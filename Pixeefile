#!/usr/bin/env pythonw
# -*- coding: utf-8 -*-
"""
defectdojo_backup_restore_with_files_tooltips.pyw

DefectDojo Backup & Restore tool (single-file) with:
 - Backup/restore of many resources including users, products, engagements, tests, findings, notes,
   development_environments, finding_templates
 - File/document/attachment backup & restore using resource-specific list/download endpoints and fallback document upload
 - FK ID mapping for restores, reuse/update/dry-run options, concurrency, retry/backoff
 - Confirmation popups before Backup and Restore
 - PyQt5 GUI with tooltips explaining UI fields and options
 - New: Restore directly from a ZIP archive (extracts JSON + documents and uses them for restore)

Requirements:
    pip install PyQt5 requests

Author: Assistant
Date: 2025-10-03 (updated)
"""

from __future__ import annotations
import sys
import os
import json
import time
import math
import threading
from collections import OrderedDict
from functools import partial
from queue import Queue
from urllib.parse import urlparse
import mimetypes
import traceback
from typing import Any, Dict, List, Optional, Tuple
import zipfile
import tempfile
import shutil

import requests
from PyQt5 import QtCore, QtGui, QtWidgets

# ---------------- Configuration defaults ----------------
DEFAULT_BASE_URL = "https://demo.defectdojo.org"
DEFAULT_TOKEN = "260e640e0c036b48a5a4519f3ed40c782cd2dfb5"

# Endpoints for resources
RESOURCE_ENDPOINTS = OrderedDict([
    ("users", "/api/v2/users/"),
    ("products", "/api/v2/products/"),
    ("engagements", "/api/v2/engagements/"),
    ("tests", "/api/v2/tests/"),
    ("findings", "/api/v2/findings/"),
    ("notes", "/api/v2/notes/"),
    ("development_environments", "/api/v2/development_environments/"),
    ("finding_templates", "/api/v2/finding_templates/"),
])

# Explicit file endpoints for attachments (per resource) - list and download templates
FILES_ENDPOINTS = {
    "engagements_list": "/api/v2/engagements/{id}/files/",
    "engagements_download": "/api/v2/engagements/{id}/files/download/{file_id}/",
    "tests_list": "/api/v2/tests/{id}/files/",
    "tests_download": "/api/v2/tests/{id}/files/download/{file_id}/",
    "findings_list": "/api/v2/findings/{id}/files/",
    "findings_download": "/api/v2/findings/{id}/files/download/{file_id}/",
}

# Generic documents endpoint (DefectDojo)
DOCUMENTS_ENDPOINT = "/api/v2/documents/"

# Restore order to satisfy FK dependencies
RESTORE_ORDER = ["users", "products", "engagements", "tests", "findings", "notes", "development_environments", "finding_templates"]

# Server-managed fields to strip before POST/PUT
SERVER_FIELDS_TO_STRIP = {
    "id", "created", "updated", "date", "last_modified", "active", "deleted", "url", "uuid"
}

# Unique-key heuristics per resource for reuse/update detection
UNIQUE_KEY_BY_RESOURCE = {
    "users": "username",
    "products": "name",
    "engagements": "name",
    "tests": "title",            # fallback to 'name' if 'title' missing
    "findings": "title",
    "notes": "title",
    "development_environments": "name",
    "finding_templates": "title"
}

# Keys that commonly contain attachments / file URLs in objects (heuristic)
POSSIBLE_ATTACHMENT_KEYS = [
    "file", "file_upload", "file_upload_url", "file_url", "attachment", "document", "url", "path", "filename"
]

# ---------------- Utility functions ----------------
def now_ts() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")

def safe_get(obj: Any, key: str, fallback=None):
    if isinstance(obj, dict):
        return obj.get(key, fallback)
    return fallback

def deep_copy(obj: Any):
    return json.loads(json.dumps(obj))

def strip_server_fields(obj: Any):
    """Recursively remove server-managed fields from dicts."""
    if isinstance(obj, dict):
        out = {}
        for k, v in obj.items():
            if k.lower() in SERVER_FIELDS_TO_STRIP:
                continue
            if isinstance(v, dict):
                out[k] = strip_server_fields(v)
            elif isinstance(v, list):
                out[k] = [strip_server_fields(i) if isinstance(i, dict) else i for i in v]
            else:
                out[k] = v
        return out
    return obj

def filename_from_url(url: str) -> str:
    try:
        p = urlparse(url)
        name = os.path.basename(p.path)
        if not name:
            name = f"download_{int(time.time() * 1000)}"
        return name
    except Exception:
        return f"download_{int(time.time() * 1000)}"

def guess_mime(fname: str) -> str:
    return mimetypes.guess_type(fname)[0] or "application/octet-stream"

# Exponential backoff wrapper around requests
def retry_request(session: requests.Session, method: str, url: str,
                  headers: Optional[Dict[str, str]] = None,
                  json_payload: Optional[Any] = None,
                  data: Optional[Any] = None,
                  files: Optional[Dict[str, Tuple[str, Any, str]]] = None,
                  timeout: int = 30,
                  max_retries: int = 3,
                  backoff_factor: float = 0.5) -> requests.Response:
    attempt = 0
    last_exc = None
    while attempt <= max_retries:
        try:
            if method.lower() == "get":
                r = session.get(url, headers=headers, timeout=timeout)
            elif method.lower() == "post":
                if files is not None:
                    r = session.post(url, headers=headers, files=files, data=data, timeout=timeout)
                elif json_payload is not None:
                    r = session.post(url, headers=headers, json=json_payload, timeout=timeout)
                else:
                    r = session.post(url, headers=headers, data=data, timeout=timeout)
            elif method.lower() == "put":
                if json_payload is not None:
                    r = session.put(url, headers=headers, json=json_payload, timeout=timeout)
                else:
                    r = session.put(url, headers=headers, data=data, timeout=timeout)
            else:
                raise ValueError("Unsupported HTTP method: " + method)
            return r
        except Exception as e:
            last_exc = e
            wait = backoff_factor * (2 ** attempt)
            time.sleep(wait)
            attempt += 1
    raise last_exc

# ---------------- Worker class for restore (Qt signals) ----------------
class RestoreWorker(QtCore.QObject):
    log_signal = QtCore.pyqtSignal(str)
    progress_signal = QtCore.pyqtSignal(int, int)  # global%, resource%
    done_signal = QtCore.pyqtSignal(str)
    error_signal = QtCore.pyqtSignal(str)

    def __init__(self, base_url: str, token: str, filepaths: List[str], options: Dict[str, Any], backup_docs_root: Optional[str] = None):
        super().__init__()
        self.base_url = base_url.rstrip("/")
        self.token = token
        self.filepaths = filepaths
        self.options = options
        self.backup_docs_root = backup_docs_root
        self._stop_flag = False

    def stop(self):
        self._stop_flag = True

    def log(self, *parts):
        self.log_signal.emit(f"[{now_ts()}] " + " ".join(str(p) for p in parts))

    def fetch_existing_by_name(self, session: requests.Session, res: str) -> Dict[str, int]:
        endpoint = RESOURCE_ENDPOINTS.get(res)
        if not endpoint:
            return {}
        url = self.base_url + endpoint
        headers = {"Authorization": f"Token {self.token}", "Accept": "application/json"}
        self.log(f"Fetching existing objects for resource '{res}' from {url} ...")
        mapping: Dict[str, int] = {}
        cur = url
        while cur and not self._stop_flag:
            r = retry_request(session, "get", cur, headers=headers,
                              max_retries=self.options.get("max_retries", 3),
                              backoff_factor=self.options.get("backoff_factor", 0.5))
            if r.status_code != 200:
                self.log(f"Warning: GET {cur} -> {r.status_code}; body={r.text[:200]}")
                break
            data = r.json()
            if isinstance(data, dict) and "results" in data:
                records = data.get("results", [])
                cur = data.get("next")
            elif isinstance(data, list):
                records = data
                cur = None
            else:
                records = [data]
                cur = None
            unique_key = UNIQUE_KEY_BY_RESOURCE.get(res)
            for rec in records:
                if not isinstance(rec, dict):
                    continue
                key_val = None
                if unique_key:
                    key_val = safe_get(rec, unique_key)
                if res == "tests" and key_val is None:
                    key_val = safe_get(rec, "name")
                if key_val is not None:
                    mapping[str(key_val)] = rec.get("id")
        self.log(f"Found {len(mapping)} existing '{res}' objects.")
        return mapping

    def remap_object_impl(self, obj: Any, mapping_store: Dict[str, Dict[int, int]]):
        """Recursively remap id references using mapping_store (resource -> old_id -> new_id)."""
        if isinstance(obj, dict):
            out = {}
            for k, v in obj.items():
                if k.endswith("_id") and isinstance(v, int):
                    prefix = k[:-3].lower()
                    candidate = None
                    if prefix + "s" in mapping_store:
                        candidate = prefix + "s"
                    elif prefix in mapping_store:
                        candidate = prefix
                    if candidate and v in mapping_store.get(candidate, {}):
                        out[k] = mapping_store[candidate][v]
                    else:
                        out[k] = v
                elif k.lower() in ("product", "engagement", "test", "user", "owner"):
                    if isinstance(v, int):
                        keyname = k.lower()
                        candidate = None
                        if keyname in ("user", "owner"):
                            candidate = "users"
                        elif keyname == "product":
                            candidate = "products"
                        elif keyname == "engagement":
                            candidate = "engagements"
                        elif keyname == "test":
                            candidate = "tests"
                        if candidate and v in mapping_store.get(candidate, {}):
                            out[k] = mapping_store[candidate][v]
                        else:
                            out[k] = v
                    elif isinstance(v, dict) and "id" in v and isinstance(v["id"], int):
                        inner = v.copy()
                        old = inner["id"]
                        keyname = k.lower()
                        candidate = None
                        if keyname in ("user", "owner"):
                            candidate = "users"
                        elif keyname == "product":
                            candidate = "products"
                        elif keyname == "engagement":
                            candidate = "engagements"
                        elif keyname == "test":
                            candidate = "tests"
                        if candidate and old in mapping_store.get(candidate, {}):
                            inner["id"] = mapping_store[candidate][old]
                        out[k] = self.remap_object_impl(inner, mapping_store)
                    else:
                        out[k] = self.remap_object_impl(v, mapping_store)
                else:
                    out[k] = self.remap_object_impl(v, mapping_store)
            return out
        elif isinstance(obj, list):
            return [self.remap_object_impl(i, mapping_store) for i in obj]
        else:
            return obj

    def remap_object(self, obj: Any, mapping_store: Dict[str, Dict[int, int]]):
        return self.remap_object_impl(obj, mapping_store)

    def download_files_via_api(self, session: requests.Session, resource: str, orig_id: Any, dest_dir: str) -> List[Dict[str, Any]]:
        """
        Uses resource-specific list & download endpoints to fetch files.
        Returns list of metadata dicts for downloaded files.
        """
        results: List[Dict[str, Any]] = []
        try:
            if resource == "engagements":
                list_ep = FILES_ENDPOINTS.get("engagements_list")
                dl_ep = FILES_ENDPOINTS.get("engagements_download")
            elif resource == "tests":
                list_ep = FILES_ENDPOINTS.get("tests_list")
                dl_ep = FILES_ENDPOINTS.get("tests_download")
            elif resource == "findings":
                list_ep = FILES_ENDPOINTS.get("findings_list")
                dl_ep = FILES_ENDPOINTS.get("findings_download")
            else:
                return results
            if not list_ep or not dl_ep:
                return results
            list_url = self.base_url + list_ep.format(id=orig_id)
            headers = {"Authorization": f"Token {self.token}", "Accept": "application/json"}
            r = retry_request(session, "get", list_url, headers=headers,
                              max_retries=self.options.get("max_retries", 3),
                              backoff_factor=self.options.get("backoff_factor", 0.5))
            if r.status_code != 200:
                self.log(f"Files list API for {resource} id={orig_id} returned {r.status_code}")
                return results
            data = r.json()
            # Expecting structure similar to { "engagement_id": 18, "files": [ {id:1, file: "url", title: "View"}, ... ] }
            files_list = []
            if isinstance(data, dict) and "files" in data:
                files_list = data.get("files", [])
            elif isinstance(data, list):
                files_list = data
            else:
                files_list = data if isinstance(data, list) else []
            os.makedirs(dest_dir, exist_ok=True)
            for fmeta in files_list:
                try:
                    file_id = fmeta.get("id")
                    remote_name = None
                    if "file" in fmeta and isinstance(fmeta.get("file"), str):
                        remote_name = os.path.basename(urlparse(fmeta.get("file")).path) if fmeta.get("file") else None
                    # construct download URL
                    dl_url = self.base_url + dl_ep.format(id=orig_id, file_id=file_id)
                    rr = retry_request(session, "get", dl_url, headers={"Authorization": f"Token {self.token}"}, max_retries=self.options.get("max_retries", 3), backoff_factor=self.options.get("backoff_factor", 0.5))
                    if rr.status_code == 200:
                        fname = remote_name or f"{resource}_{orig_id}_{file_id}"
                        # try content-disposition header
                        cd = rr.headers.get("Content-Disposition")
                        if cd and "filename=" in cd:
                            try:
                                fname = cd.split("filename=")[1].strip().strip('";')
                            except Exception:
                                pass
                        out_path = os.path.join(dest_dir, fname)
                        with open(out_path, "wb") as fh:
                            fh.write(rr.content)
                        content_type = rr.headers.get("Content-Type", guess_mime(fname))
                        results.append({"filename": fname, "path": out_path, "file_id": file_id, "content_type": content_type, "field": "files"})
                        self.log(f"Downloaded via files API: {resource} id={orig_id} file_id={file_id} -> {fname}")
                    else:
                        self.log(f"Failed to download file via API {dl_url}: HTTP {rr.status_code}")
                except Exception as e:
                    self.log(f"Exception downloading file metadata {fmeta}: {e}")
            return results
        except Exception as e:
            self.log(f"Files API download error for {resource} id={orig_id}: {e}")
            return results

    def download_attachments_for_object(self, session: requests.Session, obj: Dict[str, Any], resource_name: str, backup_docs_root: str) -> List[Dict[str, Any]]:
        """
        Scan object and attempt to download attachments. First tries resource-specific files API,
        then falls back to scanning for URLs in object fields.
        Returns list of metadata dicts for downloaded files.
        """
        results: List[Dict[str, Any]] = []
        if not isinstance(obj, dict):
            return results
        orig_id = obj.get("id", "noid")
        dest_dir = os.path.join(backup_docs_root, "documents", resource_name, str(orig_id))
        os.makedirs(dest_dir, exist_ok=True)

        # 1) Try resource-specific files API
        api_results = self.download_files_via_api(session, resource_name, orig_id, dest_dir)
        if api_results:
            results.extend(api_results)
            return results

        # 2) Fallback: scan object fields for URLs
        def scan(value, parent_key=None):
            if isinstance(value, dict):
                for kk, vv in value.items():
                    scan(vv, kk)
            elif isinstance(value, list):
                for it in value:
                    scan(it, parent_key)
            else:
                if isinstance(value, str):
                    s = value.strip()
                    if s.startswith("http://") or s.startswith("https://"):
                        url = s
                        fname = filename_from_url(url)
                        out_path = os.path.join(dest_dir, fname)
                        try:
                            r = retry_request(session, "get", url, headers=None,
                                              max_retries=self.options.get("max_retries", 3),
                                              backoff_factor=self.options.get("backoff_factor", 0.5))
                            if r.status_code == 200:
                                with open(out_path, "wb") as fh:
                                    fh.write(r.content)
                                content_type = r.headers.get("Content-Type", guess_mime(fname))
                                results.append({"filename": fname, "path": out_path, "field": parent_key, "content_type": content_type})
                                self.log(f"Downloaded attachment fallback for {resource_name} id={orig_id}: {fname}")
                            else:
                                self.log(f"Failed to download {url}: HTTP {r.status_code}")
                        except Exception as e:
                            self.log(f"Exception downloading {url}: {e}")
                    elif s.startswith("data:"):
                        self.log(f"Found data URI in field {parent_key} for {resource_name} id={orig_id} — skipping inline data URI.")
        # prioritized scan of known keys
        found_any = False
        for candidate_key in POSSIBLE_ATTACHMENT_KEYS:
            if candidate_key in obj:
                scan(obj[candidate_key], candidate_key)
                found_any = True
        if not found_any:
            scan(obj)
        return results

    def upload_files_endpoint(self, session: requests.Session, resource_name: str, target_id: int, doc_folder: str) -> List[Tuple[str, Optional[int], str]]:
        """
        Upload files to specific per-resource files API:
          /api/v2/engagements/{id}/files/
          /api/v2/tests/{id}/files/
          /api/v2/findings/{id}/files/
        Returns list of (filename, status_code, response_text).
        """
        results = []
        if resource_name == "engagements":
            endpoint_template = FILES_ENDPOINTS.get("engagements_list")
        elif resource_name == "tests":
            endpoint_template = FILES_ENDPOINTS.get("tests_list")
        elif resource_name == "findings":
            endpoint_template = FILES_ENDPOINTS.get("findings_list")
        else:
            endpoint_template = None
        if not endpoint_template:
            return results
        url = self.base_url + endpoint_template.format(id=target_id)
        headers_no_content = {"Authorization": f"Token {self.token}"}
        if not os.path.isdir(doc_folder):
            return results
        for fname in os.listdir(doc_folder):
            full = os.path.join(doc_folder, fname)
            if not os.path.isfile(full):
                continue
            files = {"file": (fname, open(full, "rb"), guess_mime(fname))}
            data = {"title": fname}
            try:
                r = retry_request(session, "post", url, headers=headers_no_content, files=files, data=data,
                                  max_retries=self.options.get("max_retries", 3),
                                  backoff_factor=self.options.get("backoff_factor", 0.5))
            except Exception as e:
                self.log(f"File upload exception for {fname} to {url}: {e}")
                results.append((fname, None, str(e)))
                continue
            try:
                resp_text = r.text
            except Exception:
                resp_text = ""
            results.append((fname, r.status_code if r is not None else None, resp_text))
            if r is not None and r.status_code in (200, 201):
                self.log(f"Uploaded file {fname} to {resource_name} files endpoint -> status {r.status_code}")
            else:
                self.log(f"Upload failed {fname} to {url}: status {getattr(r,'status_code',None)} - {resp_text[:200]}")
        return results

    def upload_documents_generic(self, session: requests.Session, doc_folder: str, attach: Dict[str, str]) -> List[Tuple[str, Optional[int], str]]:
        """
        Upload files found in doc_folder to /api/v2/documents/ as a fallback.
        attach: dict of additional form fields, e.g. {'engagement': '18'} or {'test': '5'}
        Returns list of tuples: (filename, status_code, response_text)
        """
        results = []
        url = self.base_url + DOCUMENTS_ENDPOINT
        headers_no_content = {"Authorization": f"Token {self.token}"}
        if not os.path.isdir(doc_folder):
            return results
        for fname in os.listdir(doc_folder):
            full = os.path.join(doc_folder, fname)
            if not os.path.isfile(full):
                continue
            files = {"file": (fname, open(full, "rb"), guess_mime(fname))}
            data = attach.copy()
            data["title"] = data.get("title") or fname
            try:
                r = retry_request(session, "post", url, headers=headers_no_content, files=files, data=data,
                                  max_retries=self.options.get("max_retries", 3),
                                  backoff_factor=self.options.get("backoff_factor", 0.5))
            except Exception as e:
                self.log(f"Document upload exception for {fname}: {e}")
                results.append((fname, None, str(e)))
                continue
            try:
                resp_text = r.text
            except Exception:
                resp_text = ""
            results.append((fname, r.status_code if r is not None else None, resp_text))
            if r is not None and r.status_code in (200, 201):
                self.log(f"Uploaded document {fname} to documents endpoint -> status {r.status_code}")
            else:
                self.log(f"Upload failed {fname}: status {getattr(r,'status_code',None)} - {resp_text[:200]}")
        return results

    def _maybe_attach_docs(self, session: requests.Session, resource_name: str, orig_id: Optional[int],
                           mapping_store: Dict[str, Dict[int, int]], target_id: Optional[int],
                           available_docs: Dict[str, Dict[Any, str]]):
        """
        After creating/updating/reusing an object, attempt to upload associated files/documents found in the
        backup folder for that resource/orig_id. It will try:
         1) resource-specific files endpoint (e.g., /api/v2/engagements/{id}/files/)
         2) if not available or failed, generic /api/v2/documents/ with appropriate fk field
        """
        if self.backup_docs_root is None or orig_id is None:
            return
        docs_map_for_res = available_docs.get(resource_name, {})
        folder = docs_map_for_res.get(orig_id)
        if folder is None:
            folder = docs_map_for_res.get(str(orig_id))
        if folder is None:
            return
        mapped_id = mapping_store.get(resource_name, {}).get(orig_id)
        attach_target_id = mapped_id or target_id
        if not attach_target_id:
            self.log(f"No target id available to attach documents for {resource_name} orig_id={orig_id}")
            return
        # Try resource-specific files endpoint first
        try:
            upload_results = self.upload_files_endpoint(session, resource_name, attach_target_id, folder)
            if upload_results:
                for fname, status, resp in upload_results:
                    self.log(f" Uploaded file '{fname}' -> status {status}")
                return
        except Exception as e:
            self.log(f"Resource-specific upload error: {e}")
        # Fallback: upload to generic documents endpoint with proper foreign key field
        attach_field = {}
        if resource_name == "engagements":
            attach_field["engagement"] = str(attach_target_id)
        elif resource_name == "tests":
            attach_field["test"] = str(attach_target_id)
        elif resource_name == "findings":
            attach_field["finding"] = str(attach_target_id)
        else:
            attach_field["title"] = f"imported-{resource_name}-{attach_target_id}"
        try:
            doc_results = self.upload_documents_generic(session, folder, attach_field)
            for fname, status, resp in doc_results:
                self.log(f" Document upload: '{fname}' -> status {status}")
        except Exception as e:
            self.log(f"Generic document upload error: {e}")

    def run(self):
        """Main restore run method executed in a worker thread."""
        try:
            self.log("Restore worker started.")
            session = requests.Session()
            headers = {"Authorization": f"Token {self.token}", "Accept": "application/json", "Content-Type": "application/json"}

            # Load given JSON backup files
            input_data: Dict[str, List[Dict[str, Any]]] = {}
            provided_resources = set()
            for fp in self.filepaths:
                try:
                    with open(fp, "r", encoding="utf-8") as fh:
                        data = json.load(fh)
                    name = os.path.basename(fp).split(".")[0].lower()
                    input_data[name] = data if isinstance(data, list) else [data]
                    provided_resources.add(name)
                    self.log(f"Loaded {len(input_data[name])} objects from {fp} as resource '{name}'")
                except Exception as e:
                    self.log(f"Failed to read {fp}: {e}")

            resources_to_process = [r for r in RESTORE_ORDER if r in provided_resources]
            if not resources_to_process:
                resources_to_process = list(provided_resources)

            total_resources = len(resources_to_process)
            global_step = 0

            # mapping store: mapping_store[resource][old_id] = new_id
            mapping_store: Dict[str, Dict[int, Optional[int]]] = {r: {} for r in RESOURCE_ENDPOINTS.keys()}

            # Optional: fetch existing objects on target to support reuse/update
            existing_name_maps: Dict[str, Dict[str, int]] = {}
            if self.options.get("reuse_existing") or self.options.get("update_if_exists"):
                for res in resources_to_process:
                    existing_name_maps[res] = self.fetch_existing_by_name(session, res)

            # Pre-scan backup docs root for available folders (if provided)
            available_docs: Dict[str, Dict[Any, str]] = {}
            if self.backup_docs_root:
                docs_base = os.path.join(self.backup_docs_root, "documents")
                if os.path.isdir(docs_base):
                    for res in os.listdir(docs_base):
                        res_dir = os.path.join(docs_base, res)
                        if not os.path.isdir(res_dir):
                            continue
                        for orig_id in os.listdir(res_dir):
                            folder = os.path.join(res_dir, orig_id)
                            if os.path.isdir(folder):
                                key = int(orig_id) if orig_id.isdigit() else orig_id
                                available_docs.setdefault(res, {})[key] = folder
                self.log(f"Discovered document folders for resources: {list(available_docs.keys())}")

            # Process resources one by one respecting RESTORE_ORDER
            for r_index, res in enumerate(resources_to_process, start=1):
                if self._stop_flag:
                    self.log("Stop requested. Aborting restore.")
                    self.done_signal.emit("Aborted")
                    return
                items = input_data.get(res, [])
                n = len(items)
                self.log(f"Processing resource '{res}' ({r_index}/{total_resources}) - {n} items")
                endpoint = RESOURCE_ENDPOINTS.get(res)
                if not endpoint:
                    self.log(f"No endpoint mapping for resource '{res}'. Skipping.")
                    global_step += 1
                    self.progress_signal.emit(int(global_step/total_resources*100), 0)
                    continue
                full_url = self.base_url + endpoint

                created_count = 0
                processed = 0

                # Option: use lightweight concurrency per-resource (Queue + threads)
                task_q = Queue()
                for idx, obj in enumerate(items):
                    task_q.put((idx, obj))

                concurrency = max(1, min(20, int(self.options.get("concurrency", 4))))
                results_lock = threading.Lock()

                def worker_func():
                    nonlocal created_count, processed
                    while not task_q.empty() and not self._stop_flag:
                        try:
                            idx, obj = task_q.get_nowait()
                        except Exception:
                            break
                        try:
                            orig_id = obj.get("id") if isinstance(obj, dict) else None
                            # Prepare payload: deep copy, strip server-managed fields, remap fks using mapping_store
                            payload = deep_copy(obj)
                            payload = strip_server_fields(payload)
                            payload = self.remap_object_impl(payload, mapping_store)

                            # Determine unique key
                            unique_key = UNIQUE_KEY_BY_RESOURCE.get(res)
                            unique_val = None
                            if unique_key and isinstance(payload, dict):
                                unique_val = payload.get(unique_key)
                            if res == "tests" and unique_val is None and isinstance(payload, dict):
                                unique_val = payload.get("name") or payload.get("title")

                            target_existing_id = None
                            if unique_val is not None:
                                name_map = existing_name_maps.get(res, {})
                                target_existing_id = name_map.get(str(unique_val))

                            # Reuse existing?
                            if self.options.get("reuse_existing") and target_existing_id:
                                with results_lock:
                                    if orig_id is not None:
                                        mapping_store[res][orig_id] = target_existing_id
                                    created_count += 1
                                    processed += 1
                                self.log(f"[{res}] Reused existing object '{unique_val}' -> id {target_existing_id} (orig id {orig_id})")
                                task_q.task_done()
                                # after remap, attach docs if present
                                self._maybe_attach_docs(session, res, orig_id, mapping_store, target_existing_id, available_docs)
                                # progress
                                rprog = int((processed / n) * 100) if n else 100
                                gprog = int(((r_index-1)/total_resources)*100 + (processed/n)*(100/total_resources)) if n else int((r_index/total_resources)*100)
                                self.progress_signal.emit(gprog, rprog)
                                continue

                            # Update existing?
                            if target_existing_id and self.options.get("update_if_exists"):
                                put_url = full_url + f"{target_existing_id}/"
                                try:
                                    r = retry_request(session, "put", put_url, headers=headers, json_payload=payload,
                                                      max_retries=self.options.get("max_retries", 3),
                                                      backoff_factor=self.options.get("backoff_factor", 0.5))
                                except Exception as e:
                                    self.log(f"[{res}] PUT exception for '{unique_val}': {e}")
                                    r = None
                                if r is not None and r.status_code in (200, 201):
                                    with results_lock:
                                        if orig_id is not None:
                                            mapping_store[res][orig_id] = target_existing_id
                                        created_count += 1
                                        processed += 1
                                    self.log(f"[{res}] Updated existing '{unique_val}' id={target_existing_id}")
                                else:
                                    self.log(f"[{res}] Failed to update existing '{unique_val}' (status={getattr(r, 'status_code', None)})")
                                task_q.task_done()
                                # attach docs after update
                                self._maybe_attach_docs(session, res, orig_id, mapping_store, target_existing_id, available_docs)
                                rprog = int((processed / n) * 100) if n else 100
                                gprog = int(((r_index-1)/total_resources)*100 + (processed/n)*(100/total_resources)) if n else int((r_index/total_resources)*100)
                                self.progress_signal.emit(gprog, rprog)
                                continue

                            # Dry-run?
                            if self.options.get("dry_run"):
                                simulated_new = (orig_id if orig_id is not None else - (idx + 1))
                                if orig_id is not None:
                                    with results_lock:
                                        mapping_store[res][orig_id] = simulated_new
                                        created_count += 1
                                        processed += 1
                                self.log(f"[DRY RUN] Would POST {res} unique='{unique_val}' payload_keys={list(payload.keys())}")
                                task_q.task_done()
                                # no docs uploaded in dry-run
                                rprog = int((processed / n) * 100) if n else 100
                                gprog = int(((r_index-1)/total_resources)*100 + (processed/n)*(100/total_resources)) if n else int((r_index/total_resources)*100)
                                self.progress_signal.emit(gprog, rprog)
                                continue

                            # Actual POST create
                            try:
                                r = retry_request(session, "post", full_url, headers=headers, json_payload=payload,
                                                  max_retries=self.options.get("max_retries", 3),
                                                  backoff_factor=self.options.get("backoff_factor", 0.5))
                            except Exception as e:
                                self.log(f"[{res}] POST exception for object idx {idx}: {e}")
                                r = None
                            if r is not None and r.status_code in (200, 201):
                                try:
                                    created_json = r.json()
                                    new_id = created_json.get("id")
                                except Exception:
                                    new_id = None
                                with results_lock:
                                    if orig_id is not None:
                                        mapping_store[res][orig_id] = new_id
                                    created_count += 1
                                    processed += 1
                                self.log(f"[{res}] Created '{unique_val}' -> new_id={new_id} (orig id={orig_id})")
                                # After success attach docs if present
                                self._maybe_attach_docs(session, res, orig_id, mapping_store, new_id, available_docs)
                            else:
                                self.log(f"[{res}] Failed to create '{unique_val}': status={getattr(r,'status_code', None)} body={getattr(r,'text', '')[:200]}")
                                with results_lock:
                                    processed += 1
                            # update progress
                            rprog = int((processed / n) * 100) if n else 100
                            gprog = int(((r_index-1)/total_resources)*100 + (processed/n)*(100/total_resources)) if n else int((r_index/total_resources)*100)
                            self.progress_signal.emit(gprog, rprog)
                        except Exception as exc:
                            self.log("Worker exception:", str(exc), traceback.format_exc()[:1000])
                            with results_lock:
                                processed += 1
                        finally:
                            task_q.task_done()

                # start threads
                threads = []
                for _ in range(concurrency):
                    t = threading.Thread(target=worker_func, daemon=True)
                    t.start()
                    threads.append(t)
                # wait
                task_q.join()
                for t in threads:
                    if t.is_alive():
                        t.join(timeout=0.1)

                self.log(f"Completed resource '{res}': created/mapped {created_count}/{n} items.")
                global_step += 1
                self.progress_signal.emit(int(global_step/total_resources*100), 0)

            # Final mapping summary (print some mappings)
            self.log("ID Mapping summary (limited display):")
            for r_name, mp in mapping_store.items():
                if mp:
                    self.log(f"  {r_name}: {len(mp)} mappings. First up to 50 shown:")
                    cnt = 0
                    for old, new in mp.items():
                        self.log(f"    {old} => {new}")
                        cnt += 1
                        if cnt >= 50:
                            self.log("    ... (truncated)")
                            break

            self.log("Restore finished successfully.")
            self.done_signal.emit("Restore finished")
        except Exception as e:
            tb = traceback.format_exc()
            self.log("Fatal restore error:", str(e), tb[:2000])
            self.error_signal.emit(str(e))

# ---------------- PyQt GUI ----------------
class MainWindow(QtWidgets.QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("DefectDojo Backup & Restore (Files & Docs) — Tooltips")
        self.resize(1200, 800)
        self.worker_thread: Optional[QtCore.QThread] = None
        self.worker_obj: Optional[RestoreWorker] = None
        # temporary folder used when extracting ZIP for restore; cleaned up after restore
        self._temp_restore_dir: Optional[str] = None
        self._setup_ui()

    def _setup_ui(self):
        widget = QtWidgets.QWidget()
        self.setCentralWidget(widget)
        v = QtWidgets.QVBoxLayout(widget)

        # Top settings
        grid = QtWidgets.QGridLayout()
        v.addLayout(grid)
        grid.addWidget(QtWidgets.QLabel("API Base URL:"), 0, 0)
        self.base_url_edit = QtWidgets.QLineEdit(DEFAULT_BASE_URL)
        self.base_url_edit.setToolTip("Base URL of your DefectDojo instance (e.g. https://demo.defectdojo.org).")
        grid.addWidget(self.base_url_edit, 0, 1, 1, 3)
        grid.addWidget(QtWidgets.QLabel("API Token:"), 0, 4)
        self.token_edit = QtWidgets.QLineEdit(DEFAULT_TOKEN)
        self.token_edit.setEchoMode(QtWidgets.QLineEdit.Password)
        self.token_edit.setToolTip("API token used to authenticate with the DefectDojo API. Keep it secret.")
        grid.addWidget(self.token_edit, 0, 5, 1, 2)

        # Options
        hopt = QtWidgets.QHBoxLayout()
        v.addLayout(hopt)
        self.reuse_chk = QtWidgets.QCheckBox("Reuse existing objects (match by name/username)")
        self.reuse_chk.setToolTip("If enabled, objects found on the target server that match by unique key (e.g. name or username) will be reused and not re-created.")
        self.update_chk = QtWidgets.QCheckBox("Update existing objects if found (PUT)")
        self.update_chk.setToolTip("If enabled, objects found on the target server will be updated (PUT) using backup data instead of creating duplicates.")
        self.dryrun_chk = QtWidgets.QCheckBox("Dry-run (no changes)")
        self.dryrun_chk.setToolTip("When enabled, the tool will simulate restore actions and show mappings without performing POST/PUT or file uploads.")
        hopt.addWidget(self.reuse_chk)
        hopt.addWidget(self.update_chk)
        hopt.addWidget(self.dryrun_chk)
        hopt.addStretch()

        # Retry & concurrency
        hr = QtWidgets.QHBoxLayout()
        v.addLayout(hr)
        hr.addWidget(QtWidgets.QLabel("Max retries:"))
        self.retries_spin = QtWidgets.QSpinBox()
        self.retries_spin.setMinimum(0)
        self.retries_spin.setMaximum(10)
        self.retries_spin.setValue(3)
        self.retries_spin.setToolTip("Maximum number of retry attempts for transient network errors on each request. Higher values increase resilience but slow operations.")
        hr.addWidget(self.retries_spin)
        hr.addWidget(QtWidgets.QLabel("Backoff factor (s):"))
        self.backoff_spin = QtWidgets.QDoubleSpinBox()
        self.backoff_spin.setDecimals(2)
        self.backoff_spin.setRange(0.0, 10.0)
        self.backoff_spin.setValue(0.5)
        self.backoff_spin.setToolTip("Base backoff in seconds used for exponential backoff between retries. Wait = backoff * (2 ** attempt).")
        hr.addWidget(self.backoff_spin)
        hr.addWidget(QtWidgets.QLabel("Concurrency:"))
        self.concurrency_spin = QtWidgets.QSpinBox()
        self.concurrency_spin.setMinimum(1)
        self.concurrency_spin.setMaximum(20)
        self.concurrency_spin.setValue(4)
        self.concurrency_spin.setToolTip("Number of worker threads used for concurrent POST/PUT/upload operations per resource. Increase for faster restores on fast networks; reduce if your server rate-limits.")
        hr.addWidget(self.concurrency_spin)
        hr.addStretch()

        # Backup folder + control buttons
        hb = QtWidgets.QHBoxLayout()
        v.addLayout(hb)
        hb.addWidget(QtWidgets.QLabel("Backup folder:"))
        self.backup_folder_edit = QtWidgets.QLineEdit(os.path.expanduser("~"))
        self.backup_folder_edit.setToolTip("Folder where JSON backups and downloaded documents will be saved. Documents are stored under <folder>/documents/<resource>/<orig_id>/*.") 
        hb.addWidget(self.backup_folder_edit)
        btn_choose = QtWidgets.QPushButton("Choose...")
        btn_choose.setToolTip("Open folder chooser to select backup folder.")
        btn_choose.clicked.connect(self.choose_backup_folder)
        hb.addWidget(btn_choose)
        self.backup_btn = QtWidgets.QPushButton("Backup Selected")
        self.backup_btn.setToolTip("Start backup for the selected resources. You will be prompted to confirm before backup begins.")
        self.backup_btn.clicked.connect(self.confirm_and_start_backup)
        hb.addWidget(self.backup_btn)
        self.restore_btn = QtWidgets.QPushButton("Restore Files...")
        self.restore_btn.setToolTip("Select backup JSON files and start the restore process. You will be prompted to confirm before restore begins.")
        self.restore_btn.clicked.connect(self.confirm_and_choose_restore_files)
        hb.addWidget(self.restore_btn)
        # New: Restore from ZIP button
        self.restore_zip_btn = QtWidgets.QPushButton("Restore from ZIP...")
        self.restore_zip_btn.setToolTip("Select a ZIP archive created by this tool, extract it, and restore its JSON + documents contents.")
        self.restore_zip_btn.clicked.connect(self.confirm_and_choose_restore_zip)
        hb.addWidget(self.restore_zip_btn)
        hb.addStretch()

        # Resource checkboxes
        res_h = QtWidgets.QHBoxLayout()
        v.addLayout(res_h)
        self.resource_checks: Dict[str, QtWidgets.QCheckBox] = {}
        for r in RESOURCE_ENDPOINTS.keys():
            cb = QtWidgets.QCheckBox(r.capitalize())
            cb.setChecked(True)
            cb.setToolTip(f"Include {r} in backup/restore operations. Uncheck to skip.")
            self.resource_checks[r] = cb
            res_h.addWidget(cb)
        res_h.addStretch()

        # Progress bars
        v.addWidget(QtWidgets.QLabel("Global progress:"))
        self.global_progress = QtWidgets.QProgressBar()
        self.global_progress.setToolTip("Overall progress across all selected resources (0-100%).")
        v.addWidget(self.global_progress)
        v.addWidget(QtWidgets.QLabel("Resource progress:"))
        self.resource_progress = QtWidgets.QProgressBar()
        self.resource_progress.setToolTip("Progress of the current resource being processed (0-100%).")
        v.addWidget(self.resource_progress)

        # Log area
        v.addWidget(QtWidgets.QLabel("Log:"))
        self.log_edit = QtWidgets.QPlainTextEdit()
        self.log_edit.setReadOnly(True)
        self.log_edit.setFont(QtGui.QFont("Consolas", 10))
        self.log_edit.setToolTip("Operation log. Shows details of API calls, downloads, uploads, errors, and ID mapping summaries.")
        v.addWidget(self.log_edit, 1)

        # Stop button
        hb2 = QtWidgets.QHBoxLayout()
        v.addLayout(hb2)
        self.stop_btn = QtWidgets.QPushButton("Stop")
        self.stop_btn.setEnabled(False)
        self.stop_btn.setToolTip("Stop the running backup or restore operation (best-effort). Operations already in-flight may still complete.")
        self.stop_btn.clicked.connect(self.stop_worker)
        hb2.addWidget(self.stop_btn)
        hb2.addStretch()

    def choose_backup_folder(self):
        folder = QtWidgets.QFileDialog.getExistingDirectory(self, "Select Backup Folder", self.backup_folder_edit.text())
        if folder:
            self.backup_folder_edit.setText(folder)

    def append_log(self, text: str):
        self.log_edit.appendPlainText(text)

    def set_progress(self, g: Optional[int], r: Optional[int]):
        if g is not None:
            self.global_progress.setValue(max(0, min(100, int(g))))
        if r is not None:
            self.resource_progress.setValue(max(0, min(100, int(r))))

    # ---------------- Confirmation + Backup startup ----------------
    def confirm_and_start_backup(self):
        selected = [r for r, cb in self.resource_checks.items() if cb.isChecked()]
        if not selected:
            QtWidgets.QMessageBox.warning(self, "No resources", "Select at least one resource to backup.")
            return
        folder = self.backup_folder_edit.text().strip() or os.path.expanduser("~")
        msg = f"Start backup of {len(selected)} resource(s) to folder:\n{folder}\n\nProceed?"
        reply = QtWidgets.QMessageBox.question(self, "Confirm Backup", msg, QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
        if reply == QtWidgets.QMessageBox.Yes:
            self.start_backup()
        else:
            self.append_log(f"[{now_ts()}] Backup cancelled by user.")

    def start_backup(self):
        selected = [r for r, cb in self.resource_checks.items() if cb.isChecked()]
        folder = self.backup_folder_edit.text().strip() or os.path.expanduser("~")
        token = self.token_edit.text().strip()
        base_url = self.base_url_edit.text().strip().rstrip("/")
        t = threading.Thread(target=self._backup_worker, args=(base_url, token, folder, selected), daemon=True)
        t.start()

    def _backup_worker(self, base_url: str, token: str, folder: str, resources: List[str]):
        try:
            self.append_log(f"[{now_ts()}] Starting backup to: {folder}")
            session = requests.Session()
            headers = {"Authorization": f"Token {token}", "Accept": "application/json"}
            total = len(resources)
            step = 0
            docs_root = os.path.join(folder)  # will use folder/documents/...
            for res in resources:
                self.append_log(f"[{now_ts()}] Backing up resource: {res}")
                endpoint = RESOURCE_ENDPOINTS.get(res)
                if not endpoint:
                    self.append_log(f"No endpoint for {res}, skipping.")
                    step += 1
                    self.set_progress(int(step/total*100), 0)
                    continue
                url = base_url + endpoint
                items = []
                cur = url
                while cur:
                    try:
                        r = retry_request(session, "get", cur, headers=headers, max_retries=int(self.retries_spin.value()), backoff_factor=float(self.backoff_spin.value()))
                    except Exception as e:
                        self.append_log(f" GET error: {e}")
                        break
                    if r.status_code != 200:
                        self.append_log(f" GET {cur} -> {r.status_code}: {r.text[:200]}")
                        break
                    data = r.json()
                    if isinstance(data, dict) and "results" in data:
                        items.extend(data.get("results", []))
                        cur = data.get("next")
                    elif isinstance(data, list):
                        items.extend(data)
                        cur = None
                    else:
                        items.append(data)
                        cur = None
                fname = os.path.join(folder, f"{res}.json")
                try:
                    with open(fname, "w", encoding="utf-8") as fh:
                        json.dump(items, fh, indent=2, ensure_ascii=False)
                    self.append_log(f"Wrote {len(items)} objects to {fname}")
                except Exception as e:
                    self.append_log(f"Write error for {fname}: {e}")
                # For certain resources (engagements, tests, findings) download attachments into documents folder
                if res in ("engagements", "tests", "findings"):
                    self.append_log(f"Scanning attachments for resource {res} and downloading into documents folder...")
                    for obj in items:
                        try:
                            orig_id = obj.get("id", "noid")
                            dest_dir = os.path.join(folder, "documents", res, str(orig_id))
                            os.makedirs(dest_dir, exist_ok=True)
                            # 1) Try resource-specific files API
                            api_results = []
                            try:
                                api_results = self._download_files_via_api_local(session, res, orig_id, dest_dir, base_url, token)
                            except Exception as e:
                                self.append_log(f"Files API check failed for {res} id={orig_id}: {e}")
                            if api_results:
                                # already downloaded via API
                                continue
                            # 2) Fallback: scan object fields for URLs
                            def scan_for_urls(value):
                                if isinstance(value, dict):
                                    for kk, vv in value.items():
                                        scan_for_urls(vv)
                                elif isinstance(value, list):
                                    for it in value:
                                        scan_for_urls(it)
                                else:
                                    if isinstance(value, str):
                                        s = value.strip()
                                        if s.startswith("http://") or s.startswith("https://"):
                                            url_val = s
                                            fname_dl = filename_from_url(url_val)
                                            out_path = os.path.join(dest_dir, fname_dl)
                                            try:
                                                rr = retry_request(session, "get", url_val, headers=None, max_retries=int(self.retries_spin.value()), backoff_factor=float(self.backoff_spin.value()))
                                                if rr.status_code == 200:
                                                    with open(out_path, "wb") as fh:
                                                        fh.write(rr.content)
                                                    self.append_log(f"Downloaded {fname_dl} for {res} id={orig_id}")
                                                else:
                                                    self.append_log(f"Failed to download {url_val}: HTTP {rr.status_code}")
                                            except Exception as ee:
                                                self.append_log(f"Exception downloading {url_val}: {ee}")
                            found_any = False
                            for cand in POSSIBLE_ATTACHMENT_KEYS:
                                if cand in obj:
                                    scan_for_urls(obj[cand])
                                    found_any = True
                            if not found_any:
                                scan_for_urls(obj)
                        except Exception as e:
                            self.append_log(f"Error scanning attachments for object id {obj.get('id')}: {e}")
                step += 1
                self.set_progress(int(step/total*100), 0)
            self.append_log(f"[{now_ts()}] Backup finished.")
            self.set_progress(100, 100)
        except Exception as e:
            self.append_log(f"[{now_ts()}] Fatal backup error: {e}")
            self.append_log(traceback.format_exc())

    def _download_files_via_api_local(self, session: requests.Session, resource: str, orig_id: Any, dest_dir: str, base_url: str, token: str) -> List[Dict[str, Any]]:
        """
        Local helper used in backup flow to call resource-specific files list & download endpoints.
        Mirrors RestoreWorker.download_files_via_api but simpler to avoid coupling.
        """
        results: List[Dict[str, Any]] = []
        try:
            if resource == "engagements":
                list_ep = FILES_ENDPOINTS.get("engagements_list")
                dl_ep = FILES_ENDPOINTS.get("engagements_download")
            elif resource == "tests":
                list_ep = FILES_ENDPOINTS.get("tests_list")
                dl_ep = FILES_ENDPOINTS.get("tests_download")
            elif resource == "findings":
                list_ep = FILES_ENDPOINTS.get("findings_list")
                dl_ep = FILES_ENDPOINTS.get("findings_download")
            else:
                return results
            if not list_ep or not dl_ep:
                return results
            list_url = base_url + list_ep.format(id=orig_id)
            headers = {"Authorization": f"Token {token}", "Accept": "application/json"}
            r = retry_request(session, "get", list_url, headers=headers, max_retries=int(self.retries_spin.value()), backoff_factor=float(self.backoff_spin.value()))
            if r.status_code != 200:
                return results
            data = r.json()
            files_list = []
            if isinstance(data, dict) and "files" in data:
                files_list = data.get("files", [])
            elif isinstance(data, list):
                files_list = data
            else:
                files_list = data if isinstance(data, list) else []
            os.makedirs(dest_dir, exist_ok=True)
            for fmeta in files_list:
                try:
                    file_id = fmeta.get("id")
                    remote_name = None
                    if "file" in fmeta and isinstance(fmeta.get("file"), str):
                        remote_name = os.path.basename(urlparse(fmeta.get("file")).path) if fmeta.get("file") else None
                    dl_url = base_url + dl_ep.format(id=orig_id, file_id=file_id)
                    rr = retry_request(session, "get", dl_url, headers={"Authorization": f"Token {token}"}, max_retries=int(self.retries_spin.value()), backoff_factor=float(self.backoff_spin.value()))
                    if rr.status_code == 200:
                        fname = remote_name or f"{resource}_{orig_id}_{file_id}"
                        cd = rr.headers.get("Content-Disposition")
                        if cd and "filename=" in cd:
                            try:
                                fname = cd.split("filename=")[1].strip().strip('";')
                            except Exception:
                                pass
                        out_path = os.path.join(dest_dir, fname)
                        with open(out_path, "wb") as fh:
                            fh.write(rr.content)
                        content_type = rr.headers.get("Content-Type", guess_mime(fname))
                        results.append({"filename": fname, "path": out_path, "file_id": file_id, "content_type": content_type, "field": "files"})
                        self.append_log(f"Downloaded via files API: {resource} id={orig_id} file_id={file_id} -> {fname}")
                    else:
                        self.append_log(f"Failed to download file via API {dl_url}: HTTP {rr.status_code}")
                except Exception as e:
                    self.append_log(f"Exception downloading file metadata {fmeta}: {e}")
            return results
        except Exception as e:
            self.append_log(f"Files API download error for {resource} id={orig_id}: {e}")
            return results

    # ---------------- Confirmation + Restore startup ----------------
    def confirm_and_choose_restore_files(self):
        # Ask user to select backup files
        files, _ = QtWidgets.QFileDialog.getOpenFileNames(self, "Select JSON files to restore", os.path.expanduser("~"), "JSON Files (*.json);;All Files (*)")
        if not files:
            return
        base_url = self.base_url_edit.text().strip()
        token = self.token_edit.text().strip()
        options = {
            "reuse_existing": self.reuse_chk.isChecked(),
            "update_if_exists": self.update_chk.isChecked(),
            "dry_run": self.dryrun_chk.isChecked(),
            "max_retries": int(self.retries_spin.value()),
            "backoff_factor": float(self.backoff_spin.value()),
            "concurrency": int(self.concurrency_spin.value())
        }
        folder = self.backup_folder_edit.text().strip() or None
        # confirm popup
        msg = f"Start restore of {len(files)} file(s) into server:\n{base_url}\n\nOptions:\n  Reuse existing: {options['reuse_existing']}\n  Update if exists: {options['update_if_exists']}\n  Dry-run: {options['dry_run']}\n\nProceed?"
        reply = QtWidgets.QMessageBox.question(self, "Confirm Restore", msg, QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
        if reply == QtWidgets.QMessageBox.Yes:
            self.start_restore(base_url, token, files, options, folder)
        else:
            self.append_log(f"[{now_ts()}] Restore cancelled by user.")

    def confirm_and_choose_restore_zip(self):
        # New: choose zip file containing backup (JSON files & documents/)
        zf_path, _ = QtWidgets.QFileDialog.getOpenFileName(self, "Select Backup ZIP to restore", os.path.expanduser("~"), "ZIP Archives (*.zip);;All Files (*)")
        if not zf_path:
            return
        if not os.path.isfile(zf_path):
            QtWidgets.QMessageBox.warning(self, "File missing", "Selected ZIP file does not exist.")
            return
        # Ask user for confirmation
        base_url = self.base_url_edit.text().strip()
        token = self.token_edit.text().strip()
        options = {
            "reuse_existing": self.reuse_chk.isChecked(),
            "update_if_exists": self.update_chk.isChecked(),
            "dry_run": self.dryrun_chk.isChecked(),
            "max_retries": int(self.retries_spin.value()),
            "backoff_factor": float(self.backoff_spin.value()),
            "concurrency": int(self.concurrency_spin.value())
        }
        msg = f"Restore from ZIP:\n{zf_path}\n\nTarget server: {base_url}\nOptions:\n  Reuse existing: {options['reuse_existing']}\n  Update if exists: {options['update_if_exists']}\n  Dry-run: {options['dry_run']}\n\nProceed?"
        reply = QtWidgets.QMessageBox.question(self, "Confirm Restore from ZIP", msg, QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
        if reply != QtWidgets.QMessageBox.Yes:
            self.append_log(f"[{now_ts()}] Restore-from-ZIP cancelled by user.")
            return

        # Extract ZIP to a temporary directory and find JSON files
        try:
            temp_dir = tempfile.mkdtemp(prefix="dd_restore_")
            self._temp_restore_dir = temp_dir  # remember to cleanup later
            self.append_log(f"[{now_ts()}] Extracting ZIP to temporary folder: {temp_dir}")
            with zipfile.ZipFile(zf_path, 'r') as zf:
                zf.extractall(temp_dir)
            # discover JSON files at root or within top-level folder
            candidate_jsons: List[str] = []
            # walk the extracted tree and collect files that look like the resource jsons: name.json where name in RESOURCE_ENDPOINTS
            for root, dirs, files in os.walk(temp_dir):
                for f in files:
                    lower = f.lower()
                    name_noext = os.path.splitext(lower)[0]
                    if lower.endswith(".json") and name_noext in RESOURCE_ENDPOINTS.keys():
                        candidate_jsons.append(os.path.join(root, f))
            if not candidate_jsons:
                # try fallback: any .json files at top level
                for f in os.listdir(temp_dir):
                    if f.lower().endswith(".json"):
                        candidate_jsons.append(os.path.join(temp_dir, f))
            if not candidate_jsons:
                QtWidgets.QMessageBox.warning(self, "No JSON found", "No resource JSON files were found inside the ZIP. Make sure the ZIP contains <resource>.json files.")
                # cleanup temp_dir
                try:
                    shutil.rmtree(temp_dir)
                    self._temp_restore_dir = None
                except Exception:
                    pass
                return
            # start restore using extracted json files and using the extracted documents folder as backup_docs_root
            backup_docs_root = temp_dir  # the worker will look for temp_dir/documents/<resource>/<id>/
            self.append_log(f"[{now_ts()}] Found {len(candidate_jsons)} JSON resource file(s) inside ZIP. Starting restore...")
            self.start_restore(base_url, token, candidate_jsons, options, backup_docs_root)
        except Exception as e:
            self.append_log(f"[{now_ts()}] Failed to extract or start restore from ZIP: {e}")
            if self._temp_restore_dir:
                try:
                    shutil.rmtree(self._temp_restore_dir)
                except Exception:
                    pass
                self._temp_restore_dir = None

    def start_restore(self, base_url: str, token: str, files: List[str], options: Dict[str, Any], backup_docs_root: Optional[str]):
        # disable UI controls
        self.backup_btn.setEnabled(False)
        self.restore_btn.setEnabled(False)
        self.restore_zip_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)
        self.append_log(f"[{now_ts()}] Starting restore...")

        self.worker_thread = QtCore.QThread()
        self.worker_obj = RestoreWorker(base_url, token, files, options, backup_docs_root=backup_docs_root)
        self.worker_obj.moveToThread(self.worker_thread)
        self.worker_obj.log_signal.connect(self.append_log)
        self.worker_obj.progress_signal.connect(self.set_progress)
        self.worker_obj.done_signal.connect(self.restore_done)
        self.worker_obj.error_signal.connect(self.restore_error)
        self.worker_thread.started.connect(self.worker_obj.run)
        self.worker_thread.start()

    def stop_worker(self):
        if self.worker_obj:
            self.worker_obj.stop()
            self.append_log(f"[{now_ts()}] Stop requested...")

    def restore_done(self, message: str):
        self.append_log(f"[{now_ts()}] {message}")
        self._finish_restore_cleanup()

    def restore_error(self, message: str):
        self.append_log(f"[{now_ts()}] ERROR: {message}")
        self._finish_restore_cleanup()

    def _finish_restore_cleanup(self):
        self.backup_btn.setEnabled(True)
        self.restore_btn.setEnabled(True)
        self.restore_zip_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)
        self.set_progress(100, 100)
        # cleanup temp extracted folder if present
        if self._temp_restore_dir:
            try:
                shutil.rmtree(self._temp_restore_dir)
                self.append_log(f"[{now_ts()}] Cleaned up temporary extracted folder: {self._temp_restore_dir}")
            except Exception as e:
                self.append_log(f"[{now_ts()}] Warning: failed to remove temporary folder {self._temp_restore_dir}: {e}")
            self._temp_restore_dir = None
        if self.worker_thread:
            try:
                self.worker_thread.quit()
                self.worker_thread.wait(100)
            except Exception:
                pass
            self.worker_thread = None
            self.worker_obj = None

# ---------------- Main ----------------
def main():
    app = QtWidgets.QApplication(sys.argv)
    win = MainWindow()
    win.show()
    sys.exit(app.exec_())

if __name__ == "__main__":
    main()