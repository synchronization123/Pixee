#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
defectdojo_backup_only.py

Backup-only script for DefectDojo resources + attachments.
Saves JSON and downloaded documents, then zips into:
C:\Users\Admin\Desktop\Project87\Project-3\backup\<DD-MM-YYYY>.zip

Requirements:
    pip install requests

Edit CONFIG below to change base URL, token, or backup target path.
"""

from __future__ import annotations
import os
import sys
import json
import time
import shutil
import traceback
from datetime import datetime
from urllib.parse import urlparse
from typing import Any, Dict, List, Optional

import requests

# ---------------- Configuration ----------------
CONFIG = {
    "BASE_URL": "https://demo.defectdojo.org",  # change if needed
    # Token you provided (kept here for convenience). You may also set via DD_API_TOKEN env var.
    "API_TOKEN": "260e640e0c036b48a5a4519f3ed40c782cd2dfb5",
    # Which resources to back up (keeps same set used previously)
    "RESOURCES": [
        "users",
        "products",
        "engagements",
        "tests",
        "findings",
        "notes",
        "development_environments",
        "finding_templates",
    ],
    # Where final zip will be written (Windows path requested)
    "FINAL_ZIP_DIR": r"C:\Users\Admin\Desktop\Project87\Project-3\backup",
    # Temporary working folder prefix (will be created under system temp or below FINAL_ZIP_DIR)
    "TMP_BASE": None,  # if None -> will create folder next to FINAL_ZIP_DIR named "defectdojo_backup_tmp_<timestamp>"
    # Network settings
    "MAX_RETRIES": 3,
    "BACKOFF_FACTOR": 0.5,
    "REQUEST_TIMEOUT": 60,
}

# Endpoints and file endpoints (resource-specific)
RESOURCE_ENDPOINTS = {
    "users": "/api/v2/users/",
    "products": "/api/v2/products/",
    "engagements": "/api/v2/engagements/",
    "tests": "/api/v2/tests/",
    "findings": "/api/v2/findings/",
    "notes": "/api/v2/notes/",
    "development_environments": "/api/v2/development_environments/",
    "finding_templates": "/api/v2/finding_templates/",
}

FILES_ENDPOINTS = {
    "engagements_list": "/api/v2/engagements/{id}/files/",
    "engagements_download": "/api/v2/engagements/{id}/files/download/{file_id}/",
    "tests_list": "/api/v2/tests/{id}/files/",
    "tests_download": "/api/v2/tests/{id}/files/download/{file_id}/",
    "findings_list": "/api/v2/findings/{id}/files/",
    "findings_download": "/api/v2/findings/{id}/files/download/{file_id}/",
}

# Candidate fields to scan inside JSON objects for http(s) links as fallback
POSSIBLE_ATTACHMENT_KEYS = [
    "file", "file_upload", "file_upload_url", "file_url", "attachment", "document", "url", "path", "filename"
]

# ---------------- Utilities ----------------
def now_ts() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def filename_from_url(url: str) -> str:
    try:
        p = urlparse(url)
        name = os.path.basename(p.path)
        if not name:
            name = f"download_{int(time.time() * 1000)}"
        return name
    except Exception:
        return f"download_{int(time.time() * 1000)}"

def ensure_dir(p: str) -> None:
    os.makedirs(p, exist_ok=True)

# Exponential backoff wrapper for requests with retries
def retry_request(session: requests.Session, method: str, url: str,
                  headers: Optional[Dict[str,str]] = None,
                  json_payload: Optional[Any] = None,
                  data: Optional[Any] = None,
                  files: Optional[Dict[str, tuple]] = None,
                  timeout: Optional[int] = None,
                  max_retries: int = 3,
                  backoff_factor: float = 0.5) -> requests.Response:
    attempt = 0
    last_exc = None
    to = timeout or CONFIG.get("REQUEST_TIMEOUT", 60)
    while attempt <= max_retries:
        try:
            if method.lower() == "get":
                r = session.get(url, headers=headers, timeout=to, stream=False)
            elif method.lower() == "post":
                if files is not None:
                    r = session.post(url, headers=headers, files=files, data=data, timeout=to)
                elif json_payload is not None:
                    r = session.post(url, headers=headers, json=json_payload, timeout=to)
                else:
                    r = session.post(url, headers=headers, data=data, timeout=to)
            elif method.lower() == "put":
                if json_payload is not None:
                    r = session.put(url, headers=headers, json=json_payload, timeout=to)
                else:
                    r = session.put(url, headers=headers, data=data, timeout=to)
            else:
                raise ValueError("Unsupported method: " + method)
            return r
        except Exception as e:
            last_exc = e
            wait = backoff_factor * (2 ** attempt)
            print(f"[{now_ts()}] Network error for {method.upper()} {url!r}: {e}. Retrying in {wait:.1f}s (attempt {attempt}/{max_retries})")
            time.sleep(wait)
            attempt += 1
    # if we get here, all retries exhausted
    raise last_exc

# ---------------- Backup logic ----------------
def backup_resource(session: requests.Session, base_url: str, token: str, resource: str, out_folder: str) -> int:
    """
    Fetch all items for a resource and save to out_folder/<resource>.json
    For engagements/tests/findings also download attached files into out_folder/documents/<resource>/<orig_id>/
    Returns number of items saved.
    """
    endpoint = RESOURCE_ENDPOINTS.get(resource)
    if not endpoint:
        print(f"[{now_ts()}] No endpoint defined for resource '{resource}', skipping.")
        return 0
    url = base_url.rstrip("/") + endpoint
    headers = {"Authorization": f"Token {token}", "Accept": "application/json"}
    items: List[dict] = []
    cur = url
    print(f"[{now_ts()}] Fetching resource '{resource}' from {url}")
    while cur:
        try:
            r = retry_request(session, "get", cur, headers=headers,
                              max_retries=CONFIG["MAX_RETRIES"],
                              backoff_factor=CONFIG["BACKOFF_FACTOR"])
        except Exception as e:
            print(f"[{now_ts()}] ERROR fetching {cur}: {e}")
            break
        if r.status_code != 200:
            print(f"[{now_ts()}] GET {cur} -> {r.status_code}: {r.text[:500]}")
            break
        data = r.json()
        if isinstance(data, dict) and "results" in data:
            items.extend(data.get("results", []))
            cur = data.get("next")
        elif isinstance(data, list):
            items.extend(data)
            cur = None
        else:
            items.append(data)
            cur = None
    # write JSON file
    out_json_path = os.path.join(out_folder, f"{resource}.json")
    try:
        with open(out_json_path, "w", encoding="utf-8") as fh:
            json.dump(items, fh, indent=2, ensure_ascii=False)
        print(f"[{now_ts()}] Wrote {len(items)} objects to {out_json_path}")
    except Exception as e:
        print(f"[{now_ts()}] ERROR writing {out_json_path}: {e}")
    # for attachments
    if resource in ("engagements", "tests", "findings"):
        docs_base = os.path.join(out_folder, "documents", resource)
        ensure_dir(docs_base)
        for obj in items:
            try:
                orig_id = obj.get("id")
                if orig_id is None:
                    continue
                dest_dir = os.path.join(docs_base, str(orig_id))
                ensure_dir(dest_dir)
                # 1) try resource-specific files list & download endpoints
                api_downloaded = download_files_via_api(session, base_url, token, resource, orig_id, dest_dir)
                if api_downloaded:
                    # we got files via API
                    continue
                # 2) fallback: scan object for urls
                scan_and_download_urls(session, obj, dest_dir)
            except Exception as e:
                print(f"[{now_ts()}] Error handling attachments for {resource} id={obj.get('id')}: {e}")
    return len(items)

def download_files_via_api(session: requests.Session, base_url: str, token: str, resource: str, orig_id: Any, dest_dir: str) -> List[dict]:
    """
    Use resource-specific list & download API to fetch files. Returns list of downloaded file metadata.
    """
    results = []
    try:
        if resource == "engagements":
            list_ep = FILES_ENDPOINTS.get("engagements_list")
            dl_ep = FILES_ENDPOINTS.get("engagements_download")
        elif resource == "tests":
            list_ep = FILES_ENDPOINTS.get("tests_list")
            dl_ep = FILES_ENDPOINTS.get("tests_download")
        elif resource == "findings":
            list_ep = FILES_ENDPOINTS.get("findings_list")
            dl_ep = FILES_ENDPOINTS.get("findings_download")
        else:
            return results
        if not list_ep or not dl_ep:
            return results
        list_url = base_url.rstrip("/") + list_ep.format(id=orig_id)
        headers = {"Authorization": f"Token {token}", "Accept": "application/json"}
        try:
            r = retry_request(session, "get", list_url, headers=headers,
                              max_retries=CONFIG["MAX_RETRIES"],
                              backoff_factor=CONFIG["BACKOFF_FACTOR"])
        except Exception as e:
            print(f"[{now_ts()}] Files list API error for {resource} id={orig_id}: {e}")
            return results
        if r.status_code != 200:
            # no files or not supported
            return results
        data = r.json()
        files_list = []
        if isinstance(data, dict) and "files" in data:
            files_list = data.get("files", [])
        elif isinstance(data, list):
            files_list = data
        else:
            files_list = []
        for fmeta in files_list:
            try:
                file_id = fmeta.get("id")
                remote_name = None
                if isinstance(fmeta.get("file"), str):
                    remote_name = os.path.basename(urlparse(fmeta.get("file")).path)
                dl_url = base_url.rstrip("/") + dl_ep.format(id=orig_id, file_id=file_id)
                rr = retry_request(session, "get", dl_url, headers={"Authorization": f"Token {token}"}, max_retries=CONFIG["MAX_RETRIES"], backoff_factor=CONFIG["BACKOFF_FACTOR"])
                if rr.status_code == 200:
                    fname = remote_name or f"{resource}_{orig_id}_{file_id}"
                    cd = rr.headers.get("Content-Disposition")
                    if cd and "filename=" in cd:
                        try:
                            fname = cd.split("filename=")[1].strip().strip('";')
                        except Exception:
                            pass
                    out_path = os.path.join(dest_dir, fname)
                    with open(out_path, "wb") as fh:
                        fh.write(rr.content)
                    results.append({"filename": fname, "path": out_path, "file_id": file_id})
                    print(f"[{now_ts()}] Downloaded {resource} id={orig_id} file_id={file_id} -> {out_path}")
                else:
                    print(f"[{now_ts()}] Failed to download {dl_url}: HTTP {rr.status_code}")
            except Exception as e:
                print(f"[{now_ts()}] Exception downloading file metadata {fmeta}: {e}")
        return results
    except Exception as e:
        print(f"[{now_ts()}] download_files_via_api error for {resource} id={orig_id}: {e}")
        return results

def scan_and_download_urls(session: requests.Session, obj: dict, dest_dir: str) -> None:
    """
    Scan object for http(s) links and download them to dest_dir.
    """
    def scan(value, parent_key=None):
        if isinstance(value, dict):
            for kk, vv in value.items():
                scan(vv, kk)
        elif isinstance(value, list):
            for it in value:
                scan(it, parent_key)
        else:
            if isinstance(value, str):
                s = value.strip()
                if s.startswith("http://") or s.startswith("https://"):
                    url = s
                    fname = filename_from_url(url)
                    out_path = os.path.join(dest_dir, fname)
                    try:
                        r = retry_request(session, "get", url, headers=None, max_retries=CONFIG["MAX_RETRIES"], backoff_factor=CONFIG["BACKOFF_FACTOR"])
                        if r.status_code == 200:
                            with open(out_path, "wb") as fh:
                                fh.write(r.content)
                            print(f"[{now_ts()}] Downloaded fallback URL {url} -> {out_path}")
                        else:
                            print(f"[{now_ts()}] Failed to download fallback URL {url}: HTTP {r.status_code}")
                    except Exception as e:
                        print(f"[{now_ts()}] Exception downloading fallback URL {url}: {e}")
                elif s.startswith("data:"):
                    # skipping data URIs
                    print(f"[{now_ts()}] Found inline data URI (skipped).")
    # prioritized scan for common keys (saves time)
    found = False
    for k in POSSIBLE_ATTACHMENT_KEYS:
        if k in obj:
            scan(obj[k], k)
            found = True
    if not found:
        scan(obj)

# ---------------- Final zip helper ----------------
def make_zip_from_folder(src_folder: str, dest_zip_path: str) -> None:
    """
    Create a zip file from src_folder. If dest_zip_path exists, overwrite it.
    """
    # shutil.make_archive expects base name without extension and creates .zip
    base_dir = os.path.dirname(dest_zip_path)
    base_name = os.path.splitext(os.path.basename(dest_zip_path))[0]
    tmp_base = os.path.join(base_dir, base_name + "_tmp")
    # Ensure unique temp location
    if os.path.exists(dest_zip_path):
        try:
            os.remove(dest_zip_path)
        except Exception:
            pass
    # Use shutil.make_archive by copying folder to a base path? Simpler: use shutil.make_archive with root_dir=src_folder
    # But that creates zip with name base_name in base_dir. We'll use that.
    archive_base = os.path.join(base_dir, base_name)
    # remove possible leftover
    try:
        if os.path.exists(archive_base + ".zip"):
            os.remove(archive_base + ".zip")
    except Exception:
        pass
    shutil.make_archive(archive_base, 'zip', root_dir=src_folder)
    # ensure target path exactly matches dest_zip_path
    produced = archive_base + ".zip"
    if produced != dest_zip_path:
        try:
            if os.path.exists(dest_zip_path):
                os.remove(dest_zip_path)
            shutil.move(produced, dest_zip_path)
        except Exception as e:
            print(f"[{now_ts()}] Error moving produced zip to final path: {e}")

# ---------------- Main entry ----------------
def main():
    print(f"[{now_ts()}] Starting DefectDojo backup-only script")
    base_url = CONFIG.get("BASE_URL")
    token = os.environ.get("DD_API_TOKEN") or CONFIG.get("API_TOKEN")
    if not token:
        print("API token not configured. Set in CONFIG['API_TOKEN'] or environment variable DD_API_TOKEN.")
        sys.exit(1)
    resources = CONFIG.get("RESOURCES", [])
    final_zip_dir = CONFIG.get("FINAL_ZIP_DIR")
    if not final_zip_dir:
        print("FINAL_ZIP_DIR not set in CONFIG.")
        sys.exit(1)
    ensure_dir(final_zip_dir)
    # prepare temporary working folder
    date_str = datetime.now().strftime("%d-%m-%Y")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    tmp_root_parent = final_zip_dir
    tmp_name = f"defectdojo_backup_tmp_{timestamp}"
    tmp_root = CONFIG.get("TMP_BASE") or os.path.join(tmp_root_parent, tmp_name)
    # if TMP_BASE provided and exists -> use that with timestamp subfolder
    if CONFIG.get("TMP_BASE"):
        tmp_root = os.path.join(CONFIG.get("TMP_BASE"), tmp_name)
    # create
    ensure_dir(tmp_root)
    print(f"[{now_ts()}] Temporary backup folder: {tmp_root}")
    session = requests.Session()

    # set default session headers (we still pass Authorization per-request)
    # Iterate resources
    try:
        total_resources = len(resources)
        for idx, res in enumerate(resources, start=1):
            print(f"[{now_ts()}] ({idx}/{total_resources}) Backing up resource: {res}")
            try:
                count = backup_resource(session, base_url, token, res, tmp_root)
                print(f"[{now_ts()}] Finished {res}: {count} items")
            except Exception as e:
                print(f"[{now_ts()}] Error backing up {res}: {e}")
                print(traceback.format_exc())
        # create zip
        zip_name = f"{date_str}.zip"
        dest_zip_path = os.path.join(final_zip_dir, zip_name)
        print(f"[{now_ts()}] Creating zip: {dest_zip_path} (this may take a moment)")
        make_zip_from_folder(tmp_root, dest_zip_path)
        print(f"[{now_ts()}] Backup zip created at: {dest_zip_path}")
    finally:
        # Optional: cleanup tmp folder
        try:
            print(f"[{now_ts()}] Cleaning up temporary folder: {tmp_root}")
            shutil.rmtree(tmp_root)
        except Exception as e:
            print(f"[{now_ts()}] Warning: failed to remove temp folder {tmp_root}: {e}")

    print(f"[{now_ts()}] Backup completed.")

if __name__ == "__main__":
    main()