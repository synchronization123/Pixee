#!/usr/bin/env pythonw
# -*- coding: utf-8 -*-
"""
defectdojo_backup_restore_with_preflight_and_mappings.pyw

DefectDojo Backup & Restore tool (single-file) with:
 - Backup/restore of resources including users, products, engagements, tests, findings, notes,
   development_environments, finding_templates
 - File/document/attachment backup & restore using resource-specific list/download endpoints and fallback document upload
 - FK ID mapping for restores, always compares products/users against existing target objects by name/username,
   records old_id->new_id and name->new_id mappings, optionally updates existing objects (PUT) or reuses them.
 - Preflight report: inspect selected backup JSON files vs target server and report which objects will be reused/updated/created and unresolved FK references
 - Export/Load dd_id_mappings.json support to persist and reuse mappings between runs
 - Restore order ensures products and users are processed first so engagements can be remapped to new IDs.
 - PyQt5 GUI with tooltips, retry/backoff, concurrency, confirmation popups, and ZIP restore support.

Requirements:
    pip install PyQt5 requests

Author: Assistant
Date: 2025-10-03 (updated)
"""
from __future__ import annotations
import sys
import os
import json
import time
import threading
from collections import OrderedDict
from queue import Queue
from urllib.parse import urlparse
import mimetypes
import traceback
from typing import Any, Dict, List, Optional, Tuple
import zipfile
import tempfile
import shutil

import requests
from PyQt5 import QtCore, QtGui, QtWidgets

# ---------------- Configuration defaults ----------------
DEFAULT_BASE_URL = "https://demo.defectdojo.org"
DEFAULT_TOKEN = "260e640e0c036b48a5a4519f3ed40c782cd2dfb5"

# Endpoints for resources
RESOURCE_ENDPOINTS = OrderedDict([
    ("users", "/api/v2/users/"),
    ("products", "/api/v2/products/"),
    ("engagements", "/api/v2/engagements/"),
    ("tests", "/api/v2/tests/"),
    ("findings", "/api/v2/findings/"),
    ("notes", "/api/v2/notes/"),
    ("development_environments", "/api/v2/development_environments/"),
    ("finding_templates", "/api/v2/finding_templates/"),
])

# Explicit file endpoints for attachments (per resource) - list and download templates
FILES_ENDPOINTS = {
    "engagements_list": "/api/v2/engagements/{id}/files/",
    "engagements_download": "/api/v2/engagements/{id}/files/download/{file_id}/",
    "tests_list": "/api/v2/tests/{id}/files/",
    "tests_download": "/api/v2/tests/{id}/files/download/{file_id}/",
    "findings_list": "/api/v2/findings/{id}/files/",
    "findings_download": "/api/v2/findings/{id}/files/download/{file_id}/",
}

# Generic documents endpoint (DefectDojo)
DOCUMENTS_ENDPOINT = "/api/v2/documents/"

# Restore order to satisfy FK dependencies (we'll force products/users first)
RESTORE_ORDER = ["users", "products", "engagements", "tests", "findings", "notes", "development_environments", "finding_templates"]

# Server-managed fields to strip before POST/PUT
SERVER_FIELDS_TO_STRIP = {
    "id", "created", "updated", "date", "last_modified", "active", "deleted", "url", "uuid"
}

# Unique-key heuristics per resource for reuse/update detection
UNIQUE_KEY_BY_RESOURCE = {
    "users": "username",
    "products": "name",
    "engagements": "name",
    "tests": "title",            # fallback to 'name' if 'title' missing
    "findings": "title",
    "notes": "title",
    "development_environments": "name",
    "finding_templates": "title"
}

# Keys that commonly contain attachments / file URLs in objects (heuristic)
POSSIBLE_ATTACHMENT_KEYS = [
    "file", "file_upload", "file_upload_url", "file_url", "attachment", "document", "url", "path", "filename"
]

# ---------------- Utility functions ----------------
def now_ts() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")

def safe_get(obj: Any, key: str, fallback=None):
    if isinstance(obj, dict):
        return obj.get(key, fallback)
    return fallback

def deep_copy(obj: Any):
    return json.loads(json.dumps(obj))

def strip_server_fields(obj: Any):
    """Recursively remove server-managed fields from dicts."""
    if isinstance(obj, dict):
        out = {}
        for k, v in obj.items():
            if k.lower() in SERVER_FIELDS_TO_STRIP:
                continue
            if isinstance(v, dict):
                out[k] = strip_server_fields(v)
            elif isinstance(v, list):
                out[k] = [strip_server_fields(i) if isinstance(i, dict) else i for i in v]
            else:
                out[k] = v
        return out
    return obj

def filename_from_url(url: str) -> str:
    try:
        p = urlparse(url)
        name = os.path.basename(p.path)
        if not name:
            name = f"download_{int(time.time() * 1000)}"
        return name
    except Exception:
        return f"download_{int(time.time() * 1000)}"

def guess_mime(fname: str) -> str:
    return mimetypes.guess_type(fname)[0] or "application/octet-stream"

# Exponential backoff wrapper around requests
def retry_request(session: requests.Session, method: str, url: str,
                  headers: Optional[Dict[str, str]] = None,
                  json_payload: Optional[Any] = None,
                  data: Optional[Any] = None,
                  files: Optional[Dict[str, Tuple[str, Any, str]]] = None,
                  timeout: int = 30,
                  max_retries: int = 3,
                  backoff_factor: float = 0.5) -> requests.Response:
    attempt = 0
    last_exc = None
    while attempt <= max_retries:
        try:
            if method.lower() == "get":
                r = session.get(url, headers=headers, timeout=timeout)
            elif method.lower() == "post":
                if files is not None:
                    r = session.post(url, headers=headers, files=files, data=data, timeout=timeout)
                elif json_payload is not None:
                    r = session.post(url, headers=headers, json=json_payload, timeout=timeout)
                else:
                    r = session.post(url, headers=headers, data=data, timeout=timeout)
            elif method.lower() == "put":
                if json_payload is not None:
                    r = session.put(url, headers=headers, json=json_payload, timeout=timeout)
                else:
                    r = session.put(url, headers=headers, data=data, timeout=timeout)
            else:
                raise ValueError("Unsupported HTTP method: " + method)
            return r
        except Exception as e:
            last_exc = e
            wait = backoff_factor * (2 ** attempt)
            time.sleep(wait)
            attempt += 1
    raise last_exc

# ---------------- Worker class for restore (Qt signals) ----------------
class RestoreWorker(QtCore.QObject):
    # Use 'object' for these signals to avoid PyQt trying to marshal unexpected Qt types.
    log_signal = QtCore.pyqtSignal(object)
    progress_signal = QtCore.pyqtSignal(int, int)  # global%, resource%
    done_signal = QtCore.pyqtSignal(object)
    error_signal = QtCore.pyqtSignal(object)

    def __init__(self, base_url: str, token: str, filepaths: List[str], options: Dict[str, Any], backup_docs_root: Optional[str] = None, initial_mappings: Optional[Dict[str, Any]] = None):
        super().__init__()
        self.base_url = base_url.rstrip("/")
        self.token = token
        self.filepaths = filepaths
        self.options = options
        self.backup_docs_root = backup_docs_root
        self._stop_flag = False

        # mapping helpers
        # mapping_store[resource][old_id] = new_id
        self.mapping_store: Dict[str, Dict[int, Optional[int]]] = {r: {} for r in RESOURCE_ENDPOINTS.keys()}
        # backup_name_map[resource][old_id] = old_name (from backup)
        self.backup_name_map: Dict[str, Dict[int, str]] = {}
        # name_to_newid[resource][new_name] = new_id (populated from target existing objects and created objects)
        self.name_to_newid: Dict[str, Dict[str, int]] = {}
        # existing_name_maps[resource][name] = id fetched from the server (pre-fetched)
        self.existing_name_maps: Dict[str, Dict[str, int]] = {}

        # Optionally ingest externally provided mappings (from file) before restore
        if isinstance(initial_mappings, dict):
            try:
                ms = initial_mappings.get("mapping_store") or initial_mappings.get("mappings") or {}
                for k, v in ms.items():
                    if k in RESOURCE_ENDPOINTS:
                        # ensure keys are ints
                        conv = {}
                        for oldk, newv in v.items():
                            try:
                                conv[int(oldk)] = newv
                            except Exception:
                                try:
                                    conv[int(float(oldk))] = newv
                                except Exception:
                                    pass
                        self.mapping_store[k].update(conv)
                nt = initial_mappings.get("name_to_newid") or {}
                for k, v in nt.items():
                    if k in RESOURCE_ENDPOINTS:
                        for name, nid in v.items():
                            self.name_to_newid.setdefault(k, {})[str(name)] = nid
            except Exception:
                pass

    def stop(self):
        self._stop_flag = True

    def log(self, *parts):
        # Emit plain Python string as an object (safe for queued delivery)
        try:
            msg = f"[{now_ts()}] " + " ".join(str(p) for p in parts)
        except Exception:
            msg = f"[{now_ts()}] (log formatting error)"
        self.log_signal.emit(msg)

    # helper to record name->newid mapping
    def _record_name_mapping(self, resource: str, old_id: Optional[int], new_id: Optional[int], new_obj: Optional[Dict[str,Any]] = None):
        """
        Keep name->new_id and backup old_id->old_name maps in sync.
        new_obj is the response JSON for the created/updated object (may provide the authoritative 'name' or 'username').
        """
        try:
            if new_id is None:
                return
            new_name = None
            if new_obj and isinstance(new_obj, dict):
                if resource == "products":
                    new_name = new_obj.get("name")
                elif resource == "users":
                    new_name = new_obj.get("username") or new_obj.get("name")
            if new_name is None and old_id is not None:
                old_name = self.backup_name_map.get(resource, {}).get(old_id)
                if old_name:
                    new_name = old_name
            if new_name:
                new_name = str(new_name)
                self.name_to_newid.setdefault(resource, {})[new_name] = new_id
        except Exception:
            pass

    def fetch_existing_by_name(self, session: requests.Session, res: str) -> Dict[str, int]:
        """
        Fetch existing objects for a resource from the target server and return map name -> id.
        Also populate name_to_newid (so name-based remapping works even before creating anything).
        """
        endpoint = RESOURCE_ENDPOINTS.get(res)
        result: Dict[str, int] = {}
        if not endpoint:
            return result
        url = self.base_url + endpoint
        headers = {"Authorization": f"Token {self.token}", "Accept": "application/json"}
        self.log(f"Fetching existing objects for resource '{res}' from {url} ...")
        cur = url
        while cur and not self._stop_flag:
            try:
                r = retry_request(session, "get", cur, headers=headers,
                                  max_retries=self.options.get("max_retries", 3),
                                  backoff_factor=self.options.get("backoff_factor", 0.5))
            except Exception as e:
                self.log(f"GET failed while fetching existing {res}: {e}")
                break
            if r.status_code != 200:
                self.log(f"Warning: GET {cur} -> {r.status_code}; body={r.text[:200]}")
                break
            data = r.json()
            if isinstance(data, dict) and "results" in data:
                records = data.get("results", [])
                cur = data.get("next")
            elif isinstance(data, list):
                records = data
                cur = None
            else:
                records = [data]
                cur = None
            for rec in records:
                if not isinstance(rec, dict):
                    continue
                name_val = None
                if res == "products":
                    name_val = rec.get("name")
                elif res == "users":
                    name_val = rec.get("username") or rec.get("name")
                else:
                    # generic fallback: use 'name' or 'title' if available
                    name_val = rec.get("name") or rec.get("title")
                if name_val:
                    try:
                        nid = rec.get("id")
                        result[str(name_val)] = nid
                        self.name_to_newid.setdefault(res, {})[str(name_val)] = nid
                    except Exception:
                        continue
        self.log(f"Found {len(result)} existing '{res}' objects on target.")
        return result

    def remap_object_impl(self, obj: Any, mapping_store: Dict[str, Dict[int, int]]):
        """Recursively remap id references using mapping_store (resource -> old_id -> new_id).
        If mapping by old_id is missing for product/user, try to map by exact name (old_name -> new_id).
        """
        if isinstance(obj, dict):
            out = {}
            for k, v in obj.items():
                # handle integer FK patterns like product_id
                if k.endswith("_id") and isinstance(v, int):
                    prefix = k[:-3].lower()
                    candidate = None
                    if prefix + "s" in mapping_store:
                        candidate = prefix + "s"
                    elif prefix in mapping_store:
                        candidate = prefix
                    if candidate:
                        mapped = mapping_store.get(candidate, {}).get(v)
                        if mapped is not None:
                            out[k] = mapped
                        else:
                            # fallback by name for products/users if available
                            try:
                                backup_old_name = self.backup_name_map.get(candidate, {}).get(v)
                            except Exception:
                                backup_old_name = None
                            new_id_by_name = None
                            if backup_old_name:
                                new_id_by_name = self.name_to_newid.get(candidate, {}).get(str(backup_old_name))
                                if new_id_by_name is None:
                                    new_id_by_name = self.existing_name_maps.get(candidate, {}).get(str(backup_old_name))
                            if new_id_by_name is not None:
                                out[k] = new_id_by_name
                            else:
                                out[k] = v
                    else:
                        out[k] = v
                # handle product/user/test/etc fields (either int or nested object)
                elif k.lower() in ("product", "engagement", "test", "user", "owner"):
                    if isinstance(v, int):
                        keyname = k.lower()
                        candidate = None
                        if keyname in ("user", "owner"):
                            candidate = "users"
                        elif keyname == "product":
                            candidate = "products"
                        elif keyname == "engagement":
                            candidate = "engagements"
                        elif keyname == "test":
                            candidate = "tests"
                        if candidate:
                            mapped = mapping_store.get(candidate, {}).get(v)
                            if mapped is not None:
                                out[k] = mapped
                            else:
                                try:
                                    backup_old_name = self.backup_name_map.get(candidate, {}).get(v)
                                except Exception:
                                    backup_old_name = None
                                new_id_by_name = None
                                if backup_old_name:
                                    new_id_by_name = self.name_to_newid.get(candidate, {}).get(str(backup_old_name))
                                    if new_id_by_name is None:
                                        new_id_by_name = self.existing_name_maps.get(candidate, {}).get(str(backup_old_name))
                                if new_id_by_name is not None:
                                    out[k] = new_id_by_name
                                else:
                                    out[k] = v
                        else:
                            out[k] = v
                    elif isinstance(v, dict) and "id" in v and isinstance(v["id"], int):
                        inner = v.copy()
                        old = inner["id"]
                        keyname = k.lower()
                        candidate = None
                        if keyname in ("user", "owner"):
                            candidate = "users"
                        elif keyname == "product":
                            candidate = "products"
                        elif keyname == "engagement":
                            candidate = "engagements"
                        elif keyname == "test":
                            candidate = "tests"
                        if candidate:
                            mapped = mapping_store.get(candidate, {}).get(old)
                            if mapped is not None:
                                inner["id"] = mapped
                            else:
                                # fallback by name if possible (use inner name/title/username)
                                possible_name = inner.get("name") or inner.get("title") or inner.get("username")
                                new_id_by_name = None
                                if possible_name:
                                    new_id_by_name = self.name_to_newid.get(candidate, {}).get(str(possible_name))
                                    if new_id_by_name is None:
                                        new_id_by_name = self.existing_name_maps.get(candidate, {}).get(str(possible_name))
                                if new_id_by_name is not None:
                                    inner["id"] = new_id_by_name
                        out[k] = self.remap_object_impl(inner, mapping_store)
                    else:
                        out[k] = self.remap_object_impl(v, mapping_store)
                else:
                    out[k] = self.remap_object_impl(v, mapping_store)
            return out
        elif isinstance(obj, list):
            return [self.remap_object_impl(i, mapping_store) for i in obj]
        else:
            return obj

    def download_files_via_api(self, session: requests.Session, resource: str, orig_id: Any, dest_dir: str) -> List[Dict[str, Any]]:
        """
        Uses resource-specific list & download endpoints to fetch files.
        Returns list of metadata dicts for downloaded files.
        """
        results: List[Dict[str, Any]] = []
        try:
            if resource == "engagements":
                list_ep = FILES_ENDPOINTS.get("engagements_list")
                dl_ep = FILES_ENDPOINTS.get("engagements_download")
            elif resource == "tests":
                list_ep = FILES_ENDPOINTS.get("tests_list")
                dl_ep = FILES_ENDPOINTS.get("tests_download")
            elif resource == "findings":
                list_ep = FILES_ENDPOINTS.get("findings_list")
                dl_ep = FILES_ENDPOINTS.get("findings_download")
            else:
                return results
            if not list_ep or not dl_ep:
                return results
            list_url = self.base_url + list_ep.format(id=orig_id)
            headers = {"Authorization": f"Token {self.token}", "Accept": "application/json"}
            r = retry_request(session, "get", list_url, headers=headers,
                              max_retries=self.options.get("max_retries", 3),
                              backoff_factor=self.options.get("backoff_factor", 0.5))
            if r.status_code != 200:
                self.log(f"Files list API for {resource} id={orig_id} returned {r.status_code}")
                return results
            data = r.json()
            # Expecting structure similar to { "engagement_id": 18, "files": [ {id:1, file: "url", title: "View"}, ... ] }
            files_list = []
            if isinstance(data, dict) and "files" in data:
                files_list = data.get("files", [])
            elif isinstance(data, list):
                files_list = data
            else:
                files_list = data if isinstance(data, list) else []
            os.makedirs(dest_dir, exist_ok=True)
            for fmeta in files_list:
                try:
                    file_id = fmeta.get("id")
                    remote_name = None
                    if "file" in fmeta and isinstance(fmeta.get("file"), str):
                        remote_name = os.path.basename(urlparse(fmeta.get("file")).path) if fmeta.get("file") else None
                    # construct download URL
                    dl_url = self.base_url + dl_ep.format(id=orig_id, file_id=file_id)
                    rr = retry_request(session, "get", dl_url, headers={"Authorization": f"Token {self.token}"}, max_retries=self.options.get("max_retries", 3), backoff_factor=self.options.get("backoff_factor", 0.5))
                    if rr.status_code == 200:
                        fname = remote_name or f"{resource}_{orig_id}_{file_id}"
                        # try content-disposition header
                        cd = rr.headers.get("Content-Disposition")
                        if cd and "filename=" in cd:
                            try:
                                fname = cd.split("filename=")[1].strip().strip('";')
                            except Exception:
                                pass
                        out_path = os.path.join(dest_dir, fname)
                        with open(out_path, "wb") as fh:
                            fh.write(rr.content)
                        content_type = rr.headers.get("Content-Type", guess_mime(fname))
                        results.append({"filename": fname, "path": out_path, "file_id": file_id, "content_type": content_type, "field": "files"})
                        self.log(f"Downloaded via files API: {resource} id={orig_id} file_id={file_id} -> {fname}")
                    else:
                        self.log(f"Failed to download file via API {dl_url}: HTTP {rr.status_code}")
                except Exception as e:
                    self.log(f"Exception downloading file metadata {fmeta}: {e}")
            return results
        except Exception as e:
            self.log(f"Files API download error for {resource} id={orig_id}: {e}")
            return results

    def download_attachments_for_object(self, session: requests.Session, obj: Dict[str, Any], resource_name: str, backup_docs_root: str) -> List[Dict[str, Any]]:
        """
        Scan object and attempt to download attachments. First tries resource-specific files API,
        then falls back to scanning for URLs in object fields.
        Returns list of metadata dicts for downloaded files.
        """
        results: List[Dict[str, Any]] = []
        if not isinstance(obj, dict):
            return results
        orig_id = obj.get("id", "noid")
        dest_dir = os.path.join(backup_docs_root, "documents", resource_name, str(orig_id))
        os.makedirs(dest_dir, exist_ok=True)

        # 1) Try resource-specific files API
        api_results = self.download_files_via_api(session, resource_name, orig_id, dest_dir)
        if api_results:
            results.extend(api_results)
            return results

        # 2) Fallback: scan object fields for URLs
        def scan(value, parent_key=None):
            if isinstance(value, dict):
                for kk, vv in value.items():
                    scan(vv, kk)
            elif isinstance(value, list):
                for it in value:
                    scan(it, parent_key)
            else:
                if isinstance(value, str):
                    s = value.strip()
                    if s.startswith("http://") or s.startswith("https://"):
                        url = s
                        fname = filename_from_url(url)
                        out_path = os.path.join(dest_dir, fname)
                        try:
                            r = retry_request(session, "get", url, headers=None,
                                              max_retries=self.options.get("max_retries", 3),
                                              backoff_factor=self.options.get("backoff_factor", 0.5))
                            if r.status_code == 200:
                                with open(out_path, "wb") as fh:
                                    fh.write(r.content)
                                content_type = r.headers.get("Content-Type", guess_mime(fname))
                                results.append({"filename": fname, "path": out_path, "field": parent_key, "content_type": content_type})
                                self.log(f"Downloaded attachment fallback for {resource_name} id={orig_id}: {fname}")
                            else:
                                self.log(f"Failed to download {url}: HTTP {r.status_code}")
                        except Exception as e:
                            self.log(f"Exception downloading {url}: {e}")
                    elif s.startswith("data:"):
                        self.log(f"Found data URI in field {parent_key} for {resource_name} id={orig_id} — skipping inline data URI.")
        # prioritized scan of known keys
        found_any = False
        for candidate_key in POSSIBLE_ATTACHMENT_KEYS:
            if candidate_key in obj:
                scan(obj[candidate_key], candidate_key)
                found_any = True
        if not found_any:
            scan(obj)
        return results

    def upload_files_endpoint(self, session: requests.Session, resource_name: str, target_id: int, doc_folder: str) -> List[Tuple[str, Optional[int], str]]:
        """
        Upload files to specific per-resource files API:
          /api/v2/engagements/{id}/files/
          /api/v2/tests/{id}/files/
          /api/v2/findings/{id}/files/
        Returns list of (filename, status_code, response_text).
        """
        results = []
        if resource_name == "engagements":
            endpoint_template = FILES_ENDPOINTS.get("engagements_list")
        elif resource_name == "tests":
            endpoint_template = FILES_ENDPOINTS.get("tests_list")
        elif resource_name == "findings":
            endpoint_template = FILES_ENDPOINTS.get("findings_list")
        else:
            endpoint_template = None
        if not endpoint_template:
            return results
        url = self.base_url + endpoint_template.format(id=target_id)
        headers_no_content = {"Authorization": f"Token {self.token}"}
        if not os.path.isdir(doc_folder):
            return results
        for fname in os.listdir(doc_folder):
            full = os.path.join(doc_folder, fname)
            if not os.path.isfile(full):
                continue
            files = {"file": (fname, open(full, "rb"), guess_mime(fname))}
            data = {"title": fname}
            try:
                r = retry_request(session, "post", url, headers=headers_no_content, files=files, data=data,
                                  max_retries=self.options.get("max_retries", 3),
                                  backoff_factor=self.options.get("backoff_factor", 0.5))
            except Exception as e:
                self.log(f"File upload exception for {fname} to {url}: {e}")
                results.append((fname, None, str(e)))
                continue
            try:
                resp_text = r.text
            except Exception:
                resp_text = ""
            results.append((fname, r.status_code if r is not None else None, resp_text))
            if r is not None and r.status_code in (200, 201):
                self.log(f"Uploaded file {fname} to {resource_name} files endpoint -> status {r.status_code}")
            else:
                self.log(f"Upload failed {fname} to {url}: status {getattr(r,'status_code',None)} - {resp_text[:200]}")
        return results

    def upload_documents_generic(self, session: requests.Session, doc_folder: str, attach: Dict[str, str]) -> List[Tuple[str, Optional[int], str]]:
        """
        Upload files found in doc_folder to /api/v2/documents/ as a fallback.
        attach: dict of additional form fields, e.g. {'engagement': '18'} or {'test': '5'}
        Returns list of tuples: (filename, status_code, response_text)
        """
        results = []
        url = self.base_url + DOCUMENTS_ENDPOINT
        headers_no_content = {"Authorization": f"Token {self.token}"}
        if not os.path.isdir(doc_folder):
            return results
        for fname in os.listdir(doc_folder):
            full = os.path.join(doc_folder, fname)
            if not os.path.isfile(full):
                continue
            files = {"file": (fname, open(full, "rb"), guess_mime(fname))}
            data = attach.copy()
            data["title"] = data.get("title") or fname
            try:
                r = retry_request(session, "post", url, headers=headers_no_content, files=files, data=data,
                                  max_retries=self.options.get("max_retries", 3),
                                  backoff_factor=self.options.get("backoff_factor", 0.5))
            except Exception as e:
                self.log(f"Document upload exception for {fname}: {e}")
                results.append((fname, None, str(e)))
                continue
            try:
                resp_text = r.text
            except Exception:
                resp_text = ""
            results.append((fname, r.status_code if r is not None else None, resp_text))
            if r is not None and r.status_code in (200, 201):
                self.log(f"Uploaded document {fname} to documents endpoint -> status {r.status_code}")
            else:
                self.log(f"Upload failed {fname}: status {getattr(r,'status_code',None)} - {resp_text[:200]}")
        return results

    def _maybe_attach_docs(self, session: requests.Session, resource_name: str, orig_id: Optional[int],
                           mapping_store: Dict[str, Dict[int, int]], target_id: Optional[int],
                           available_docs: Dict[str, Dict[Any, str]]):
        """
        After creating/updating/reusing an object, attempt to upload associated files/documents found in the
        backup folder for that resource/orig_id. It will try:
         1) resource-specific files endpoint (e.g., /api/v2/engagements/{id}/files/)
         2) if not available or failed, generic /api/v2/documents/ with appropriate fk field
        """
        if self.backup_docs_root is None or orig_id is None:
            return
        docs_map_for_res = available_docs.get(resource_name, {})
        folder = docs_map_for_res.get(orig_id)
        if folder is None:
            folder = docs_map_for_res.get(str(orig_id))
        if folder is None:
            return
        mapped_id = mapping_store.get(resource_name, {}).get(orig_id)
        attach_target_id = mapped_id or target_id
        if not attach_target_id:
            self.log(f"No target id available to attach documents for {resource_name} orig_id={orig_id}")
            return
        # Try resource-specific files endpoint first
        try:
            upload_results = self.upload_files_endpoint(session, resource_name, attach_target_id, folder)
            if upload_results:
                for fname, status, resp in upload_results:
                    self.log(f" Uploaded file '{fname}' -> status {status}")
                return
        except Exception as e:
            self.log(f"Resource-specific upload error: {e}")
        # Fallback: upload to generic documents endpoint with proper foreign key field
        attach_field = {}
        if resource_name == "engagements":
            attach_field["engagement"] = str(attach_target_id)
        elif resource_name == "tests":
            attach_field["test"] = str(attach_target_id)
        elif resource_name == "findings":
            attach_field["finding"] = str(attach_target_id)
        else:
            attach_field["title"] = f"imported-{resource_name}-{attach_target_id}"
        try:
            doc_results = self.upload_documents_generic(session, folder, attach_field)
            for fname, status, resp in doc_results:
                self.log(f" Document upload: '{fname}' -> status {status}")
        except Exception as e:
            self.log(f"Generic document upload error: {e}")

    def run(self):
        """Main restore run method executed in a worker thread."""
        try:
            self.log("Restore worker started.")
            session = requests.Session()
            headers = {"Authorization": f"Token {self.token}", "Accept": "application/json", "Content-Type": "application/json"}

            # Load given JSON backup files
            input_data: Dict[str, List[Dict[str, Any]]] = {}
            provided_resources = set()
            for fp in self.filepaths:
                try:
                    with open(fp, "r", encoding="utf-8") as fh:
                        data = json.load(fh)
                    name = os.path.basename(fp).split(".")[0].lower()
                    input_data[name] = data if isinstance(data, list) else [data]
                    provided_resources.add(name)
                    self.log(f"Loaded {len(input_data[name])} objects from {fp} as resource '{name}'")
                except Exception as e:
                    self.log(f"Failed to read {fp}: {e}")

            # Build backup_name_map for products and users to allow name-based fallback mapping
            # backup_name_map[resource][old_id] = old_name (product.name, user.username)
            for resource in ("products", "users"):
                self.backup_name_map[resource] = {}
                self.name_to_newid.setdefault(resource, {})
                if resource in input_data:
                    for obj in input_data[resource]:
                        try:
                            old_id = obj.get("id")
                            if old_id is None:
                                continue
                            if resource == "products":
                                name_val = obj.get("name")
                            else:  # users
                                name_val = obj.get("username") or obj.get("name")
                            if name_val:
                                self.backup_name_map[resource][old_id] = str(name_val)
                        except Exception:
                            continue

            # Force products and users to be processed first if present
            resources_to_process = []
            priority = ["products", "users"]
            for p in priority:
                if p in provided_resources:
                    resources_to_process.append(p)
            # then remaining in RESTORE_ORDER preserving that order
            for r in RESTORE_ORDER:
                if r in provided_resources and r not in resources_to_process:
                    resources_to_process.append(r)
            # fallback: any provided resources not in RESTORE_ORDER
            for r in provided_resources:
                if r not in resources_to_process:
                    resources_to_process.append(r)

            total_resources = len(resources_to_process)
            global_step = 0

            # mapping store: mapping_store[resource][old_id] = new_id
            mapping_store: Dict[str, Dict[int, Optional[int]]] = self.mapping_store

            # Pre-fetch existing objects (name -> id) for products and users ALWAYS,
            # so we can map old backup IDs to existing target IDs by exact name match.
            # We also fetch for other resources if the user requested reuse/update.
            for res in ("products", "users"):
                try:
                    self.existing_name_maps[res] = self.fetch_existing_by_name(session, res)
                except Exception as e:
                    self.log(f"Failed to fetch existing {res}: {e}")
                    self.existing_name_maps[res] = {}

            if self.options.get("reuse_existing") or self.options.get("update_if_exists"):
                for res in resources_to_process:
                    if res in ("products", "users"):
                        continue
                    try:
                        self.existing_name_maps[res] = self.fetch_existing_by_name(session, res)
                    except Exception:
                        self.existing_name_maps[res] = {}

            # Pre-scan backup docs root for available folders (if provided)
            available_docs: Dict[str, Dict[Any, str]] = {}
            if self.backup_docs_root:
                docs_base = os.path.join(self.backup_docs_root, "documents")
                if os.path.isdir(docs_base):
                    for res in os.listdir(docs_base):
                        res_dir = os.path.join(docs_base, res)
                        if not os.path.isdir(res_dir):
                            continue
                        for orig_id in os.listdir(res_dir):
                            folder = os.path.join(res_dir, orig_id)
                            if os.path.isdir(folder):
                                key = int(orig_id) if orig_id.isdigit() else orig_id
                                available_docs.setdefault(res, {})[key] = folder
                self.log(f"Discovered document folders for resources: {list(available_docs.keys())}")

            # Process resources one by one respecting resources_to_process
            for r_index, res in enumerate(resources_to_process, start=1):
                if self._stop_flag:
                    self.log("Stop requested. Aborting restore.")
                    self.done_signal.emit("Aborted")
                    return
                items = input_data.get(res, [])
                n = len(items)
                self.log(f"Processing resource '{res}' ({r_index}/{total_resources}) - {n} items")
                endpoint = RESOURCE_ENDPOINTS.get(res)
                if not endpoint:
                    self.log(f"No endpoint mapping for resource '{res}'. Skipping.")
                    global_step += 1
                    self.progress_signal.emit(int(global_step/total_resources*100), 0)
                    continue
                full_url = self.base_url + endpoint

                created_count = 0
                processed = 0

                # Option: use lightweight concurrency per-resource (Queue + threads)
                task_q = Queue()
                for idx, obj in enumerate(items):
                    task_q.put((idx, obj))

                concurrency = max(1, min(20, int(self.options.get("concurrency", 4))))
                results_lock = threading.Lock()

                def worker_func():
                    nonlocal created_count, processed
                    while not task_q.empty() and not self._stop_flag:
                        try:
                            idx, obj = task_q.get_nowait()
                        except Exception:
                            break
                        try:
                            orig_id = obj.get("id") if isinstance(obj, dict) else None
                            # Prepare payload: deep copy, strip server-managed fields, remap fks using mapping_store
                            payload = deep_copy(obj)
                            payload = strip_server_fields(payload)
                            payload = self.remap_object_impl(payload, mapping_store)

                            # Determine unique key and unique value (for matching)
                            unique_key = UNIQUE_KEY_BY_RESOURCE.get(res)
                            unique_val = None
                            if unique_key and isinstance(payload, dict):
                                unique_val = payload.get(unique_key)
                            if res == "tests" and unique_val is None and isinstance(payload, dict):
                                unique_val = payload.get("name") or payload.get("title")

                            # Check for existing object by name from pre-fetched maps (always check for products/users)
                            target_existing_id = None
                            # Prefer matching by unique key value if present and in the existing maps
                            if unique_val is not None:
                                existing_map = self.existing_name_maps.get(res, {})
                                if str(unique_val) in existing_map:
                                    target_existing_id = existing_map.get(str(unique_val))

                            # For products/users we also use backup_name_map to match by old id->old name
                            if res in ("products", "users") and orig_id is not None and target_existing_id is None:
                                # old name from backup
                                old_name = self.backup_name_map.get(res, {}).get(orig_id)
                                if old_name:
                                    existing_map = self.existing_name_maps.get(res, {})
                                    if existing_map and str(old_name) in existing_map:
                                        target_existing_id = existing_map.get(str(old_name))

                            # If a target existing id is found, always record mapping (old_id -> existing_id)
                            if target_existing_id:
                                with results_lock:
                                    if orig_id is not None:
                                        mapping_store[res][orig_id] = target_existing_id
                                    created_count += 1
                                    processed += 1
                                # If update_if_exists requested, attempt PUT to update the existing object
                                if self.options.get("update_if_exists"):
                                    put_url = full_url + f"{target_existing_id}/"
                                    try:
                                        r = retry_request(session, "put", put_url, headers=headers, json_payload=payload,
                                                          max_retries=self.options.get("max_retries", 3),
                                                          backoff_factor=self.options.get("backoff_factor", 0.5))
                                    except Exception as e:
                                        self.log(f"[{res}] PUT exception for existing '{unique_val}': {e}")
                                        r = None
                                    if r is not None and r.status_code in (200, 201):
                                        try:
                                            resp_json = r.json()
                                        except Exception:
                                            resp_json = None
                                        self._record_name_mapping(res, orig_id, target_existing_id, new_obj=resp_json)
                                        self.log(f"[{res}] Updated existing '{unique_val}' id={target_existing_id}")
                                    else:
                                        self.log(f"[{res}] Failed to update existing '{unique_val}' (status={getattr(r,'status_code',None)})")
                                else:
                                    # reuse existing (no PUT), but record name mapping as we have old->existing mapping
                                    self._record_name_mapping(res, orig_id, target_existing_id, new_obj=None)
                                    self.log(f"[{res}] Reusing existing object '{unique_val}' -> id {target_existing_id} (orig id {orig_id})")
                                task_q.task_done()
                                # attach docs if present
                                self._maybe_attach_docs(session, res, orig_id, mapping_store, target_existing_id, available_docs)
                                # progress
                                rprog = int((processed / n) * 100) if n else 100
                                gprog = int(((r_index-1)/total_resources)*100 + (processed/n)*(100/total_resources)) if n else int((r_index/total_resources)*100)
                                self.progress_signal.emit(gprog, rprog)
                                continue

                            # Re-check mapping_store in case some earlier created object mapped this orig_id
                            if orig_id is not None and mapping_store.get(res, {}).get(orig_id):
                                with results_lock:
                                    processed += 1
                                task_q.task_done()
                                rprog = int((processed / n) * 100) if n else 100
                                gprog = int(((r_index-1)/total_resources)*100 + (processed/n)*(100/total_resources)) if n else int((r_index/total_resources)*100)
                                self.progress_signal.emit(gprog, rprog)
                                continue

                            # Dry-run?
                            if self.options.get("dry_run"):
                                simulated_new = (orig_id if orig_id is not None else - (idx + 1))
                                if orig_id is not None:
                                    with results_lock:
                                        mapping_store[res][orig_id] = simulated_new
                                        created_count += 1
                                        processed += 1
                                self.log(f"[DRY RUN] Would POST {res} unique='{unique_val}' payload_keys={list(payload.keys())}")
                                task_q.task_done()
                                # no docs uploaded in dry-run
                                rprog = int((processed / n) * 100) if n else 100
                                gprog = int(((r_index-1)/total_resources)*100 + (processed/n)*(100/total_resources)) if n else int((r_index/total_resources)*100)
                                self.progress_signal.emit(gprog, rprog)
                                continue

                            # Actual POST create
                            try:
                                r = retry_request(session, "post", full_url, headers=headers, json_payload=payload,
                                                  max_retries=self.options.get("max_retries", 3),
                                                  backoff_factor=self.options.get("backoff_factor", 0.5))
                            except Exception as e:
                                self.log(f"[{res}] POST exception for object idx {idx}: {e}")
                                r = None
                            if r is not None and r.status_code in (200, 201):
                                try:
                                    created_json = r.json()
                                    new_id = created_json.get("id")
                                except Exception:
                                    created_json = None
                                    new_id = None
                                with results_lock:
                                    if orig_id is not None:
                                        mapping_store[res][orig_id] = new_id
                                    created_count += 1
                                    processed += 1
                                self.log(f"[{res}] Created '{unique_val}' -> new_id={new_id} (orig id={orig_id})")
                                # record name mapping
                                self._record_name_mapping(res, orig_id, new_id, new_obj=created_json)
                                # After success attach docs if present
                                self._maybe_attach_docs(session, res, orig_id, mapping_store, new_id, available_docs)
                            else:
                                self.log(f"[{res}] Failed to create '{unique_val}': status={getattr(r,'status_code', None)} body={getattr(r,'text', '')[:200]}")
                                with results_lock:
                                    processed += 1
                            # update progress
                            rprog = int((processed / n) * 100) if n else 100
                            gprog = int(((r_index-1)/total_resources)*100 + (processed/n)*(100/total_resources)) if n else int((r_index/total_resources)*100)
                            self.progress_signal.emit(gprog, rprog)
                        except Exception as exc:
                            self.log("Worker exception:", str(exc), traceback.format_exc()[:1000])
                            with results_lock:
                                processed += 1
                        finally:
                            task_q.task_done()

                # start threads
                threads = []
                for _ in range(concurrency):
                    t = threading.Thread(target=worker_func, daemon=True)
                    t.start()
                    threads.append(t)
                # wait
                task_q.join()
                for t in threads:
                    if t.is_alive():
                        t.join(timeout=0.1)

                self.log(f"Completed resource '{res}': created/mapped {created_count}/{n} items.")
                global_step += 1
                self.progress_signal.emit(int(global_step/total_resources*100), 0)

            # Final mapping summary (print some mappings)
            self.log("ID Mapping summary (limited display):")
            for r_name, mp in mapping_store.items():
                if mp:
                    self.log(f"  {r_name}: {len(mp)} mappings. First up to 50 shown:")
                    cnt = 0
                    for old, new in mp.items():
                        self.log(f"    {old} => {new}")
                        cnt += 1
                        if cnt >= 50:
                            self.log("    ... (truncated)")
                            break

            # Persist mapping_store for audit / reuse
            try:
                mappings_out = {
                    "mapping_store": mapping_store,
                    "name_to_newid": self.name_to_newid,
                    "backup_name_map": self.backup_name_map
                }
                out_root = self.backup_docs_root or os.getcwd()
                out_path = os.path.join(out_root, "dd_id_mappings.json")
                with open(out_path, "w", encoding="utf-8") as fh:
                    json.dump(mappings_out, fh, indent=2, ensure_ascii=False)
                self.log(f"Saved ID mappings to {out_path}")
            except Exception as e:
                self.log("Failed to persist mappings:", e)

            self.log("Restore finished successfully.")
            self.done_signal.emit("Restore finished")
        except Exception as e:
            tb = traceback.format_exc()
            self.log("Fatal restore error:", str(e), tb[:2000])
            self.error_signal.emit(str(e))

# ---------------- Preflight analyzer (runs in main thread) ----------------
def analyze_preflight(base_url: str, token: str, files: List[str], options: Dict[str, Any], initial_mappings: Optional[Dict[str, Any]] = None) -> str:
    """
    Performs a preflight analysis of the selected backup JSON files against the target server.
    Returns a multi-line text summary.
    This runs in the calling thread (Main thread) and should be used with care (network IO).
    """
    summary_lines: List[str] = []
    try:
        session = requests.Session()
        headers = {"Authorization": f"Token {token}", "Accept": "application/json"}
        input_data: Dict[str, List[Dict[str, Any]]] = {}
        provided_resources = set()
        for fp in files:
            try:
                with open(fp, "r", encoding="utf-8") as fh:
                    data = json.load(fh)
                name = os.path.basename(fp).split(".")[0].lower()
                input_data[name] = data if isinstance(data, list) else [data]
                provided_resources.add(name)
                summary_lines.append(f"Loaded {len(input_data[name])} objects for resource '{name}' from {fp}")
            except Exception as e:
                summary_lines.append(f"Failed to read {fp}: {e}")
        # prepare initial mappings if provided
        mapping_store: Dict[str, Dict[int, int]] = {r: {} for r in RESOURCE_ENDPOINTS.keys()}
        name_to_newid: Dict[str, Dict[str, int]] = {}
        if isinstance(initial_mappings, dict):
            ms = initial_mappings.get("mapping_store") or initial_mappings.get("mappings") or {}
            for k, v in ms.items():
                if k in RESOURCE_ENDPOINTS:
                    conv = {}
                    for oldk, newv in v.items():
                        try:
                            conv[int(oldk)] = newv
                        except Exception:
                            continue
                    mapping_store[k].update(conv)
            nt = initial_mappings.get("name_to_newid") or {}
            for k, v in nt.items():
                if k in RESOURCE_ENDPOINTS:
                    for name, nid in v.items():
                        name_to_newid.setdefault(k, {})[str(name)] = nid

        # fetch existing name maps for products/users always, and optionally for others
        existing_name_maps: Dict[str, Dict[str, int]] = {}
        for res in ("products", "users"):
            try:
                endpoint = RESOURCE_ENDPOINTS.get(res)
                if not endpoint:
                    existing_name_maps[res] = {}
                    continue
                cur = base_url.rstrip("/") + endpoint
                existing_name_maps[res] = {}
                while cur:
                    r = retry_request(session, "get", cur, headers=headers, max_retries=int(options.get("max_retries", 3)), backoff_factor=float(options.get("backoff_factor", 0.5)))
                    if r.status_code != 200:
                        break
                    data = r.json()
                    if isinstance(data, dict) and "results" in data:
                        records = data.get("results", [])
                        cur = data.get("next")
                    elif isinstance(data, list):
                        records = data
                        cur = None
                    else:
                        records = [data]
                        cur = None
                    for rec in records:
                        if not isinstance(rec, dict):
                            continue
                        name_val = rec.get("name") if res == "products" else (rec.get("username") or rec.get("name"))
                        if name_val:
                            existing_name_maps[res][str(name_val)] = rec.get("id")
                            name_to_newid.setdefault(res, {})[str(name_val)] = rec.get("id")
            except Exception as e:
                summary_lines.append(f"Warning: failed to fetch existing {res}: {e}")
                existing_name_maps[res] = {}

        # Build backup old-id -> name maps
        backup_name_map: Dict[str, Dict[int, str]] = {}
        for resource in ("products", "users"):
            backup_name_map[resource] = {}
            if resource in input_data:
                for obj in input_data[resource]:
                    old_id = obj.get("id")
                    if old_id is None:
                        continue
                    name_val = obj.get("name") if resource == "products" else (obj.get("username") or obj.get("name"))
                    if name_val:
                        backup_name_map[resource][old_id] = str(name_val)

        # Determine for each resource/object whether reuse/update/create, and unresolved FKs
        summary_counts = {}
        unresolved_fk_examples: List[str] = []
        for res, items in input_data.items():
            will_reuse = 0
            will_update = 0
            will_create = 0
            unresolved_count = 0
            for obj in items:
                old_id = obj.get("id")
                unique_key = UNIQUE_KEY_BY_RESOURCE.get(res)
                unique_val = None
                if unique_key:
                    unique_val = obj.get(unique_key)
                if res in ("products", "users"):
                    # try exact name match against existing_name_maps
                    name_val = backup_name_map.get(res, {}).get(old_id) or unique_val
                    if name_val and name_val in existing_name_maps.get(res, {}):
                        will_reuse += 1
                        if options.get("update_if_exists"):
                            will_update += 1
                    else:
                        will_create += 1
                else:
                    # For other resources, check referenced FKs that are likely important (product_id, user/owner)
                    # We'll check product_id and user/owner keys
                    unresolved_here = False
                    # product_id
                    pid = None
                    if isinstance(obj, dict):
                        pid = obj.get("product_id") or obj.get("product")
                        # if nested dict product: {id:...}
                        pfield = obj.get("product")
                        if isinstance(pfield, dict):
                            pid = pfield.get("id")
                    if pid:
                        mapped = mapping_store.get("products", {}).get(pid) or name_to_newid.get("products", {}).get(str(backup_name_map.get("products", {}).get(pid))) or existing_name_maps.get("products", {}).get(str(backup_name_map.get("products", {}).get(pid)))
                        if mapped is None:
                            unresolved_here = True
                    # user/owner
                    uid = None
                    for key in ("user", "owner", "lead", "created_by", "reporter"):
                        if isinstance(obj, dict) and key in obj:
                            val = obj.get(key)
                            if isinstance(val, dict):
                                uid = val.get("id") or uid
                            elif isinstance(val, int):
                                uid = val or uid
                    if uid:
                        mapped = mapping_store.get("users", {}).get(uid) or name_to_newid.get("users", {}).get(str(backup_name_map.get("users", {}).get(uid))) or existing_name_maps.get("users", {}).get(str(backup_name_map.get("users", {}).get(uid)))
                        if mapped is None:
                            unresolved_here = True
                    if unresolved_here:
                        unresolved_count += 1
                        if len(unresolved_fk_examples) < 20:
                            unresolved_fk_examples.append(f"{res} old_id={old_id} maybe unresolved (product_id={pid} user={uid})")
                    # classify as create (we didn't detect reuse/update logic for general resources in preflight)
                    will_create += 1
            summary_counts[res] = {"reuse": will_reuse, "update": will_update, "create": will_create, "unresolved_fk": unresolved_count}
        # Build summary text
        summary_lines.append("\nPreflight Summary:")
        for res, c in summary_counts.items():
            summary_lines.append(f" - {res}: reuse={c['reuse']} update={c['update']} create={c['create']} unresolved_fk={c['unresolved_fk']}")
        if unresolved_fk_examples:
            summary_lines.append("\nUnresolved FK examples (first up to 20):")
            summary_lines.extend(unresolved_fk_examples)
        else:
            summary_lines.append("\nNo unresolved FK references detected in the scanned resources (based on name/old-id heuristics).")
        summary_lines.append("\nNotes:")
        summary_lines.append(" - Products and users are always compared to existing target objects by exact name/username.")
        summary_lines.append(" - If you load a prior dd_id_mappings.json via 'Load Mappings', those mappings are used to resolve references.")
        summary_lines.append(" - Preflight is heuristic-based; always review the mapping output and consider a dry-run before full restore.")
    except Exception as e:
        summary_lines.append(f"Preflight error: {e}")
        summary_lines.append(traceback.format_exc()[:2000])
    return "\n".join(summary_lines)

# ---------------- PyQt GUI ----------------
class MainWindow(QtWidgets.QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("DefectDojo Backup & Restore — Preflight & Mappings")
        self.resize(1200, 860)
        self.worker_thread: Optional[QtCore.QThread] = None
        self.worker_obj: Optional[RestoreWorker] = None
        # temporary folder used when extracting ZIP for restore; cleaned up after restore
        self._temp_restore_dir: Optional[str] = None
        # last mappings loaded from file (kept in memory for starting worker)
        self._loaded_mappings: Optional[Dict[str, Any]] = None
        # last mappings produced by worker (for Export)
        self._last_mappings: Optional[Dict[str, Any]] = None
        self._setup_ui()

    def _setup_ui(self):
        widget = QtWidgets.QWidget()
        self.setCentralWidget(widget)
        v = QtWidgets.QVBoxLayout(widget)

        # Top settings
        grid = QtWidgets.QGridLayout()
        v.addLayout(grid)
        grid.addWidget(QtWidgets.QLabel("API Base URL:"), 0, 0)
        self.base_url_edit = QtWidgets.QLineEdit(DEFAULT_BASE_URL)
        self.base_url_edit.setToolTip("Base URL of your DefectDojo instance (e.g. https://demo.defectdojo.org).")
        grid.addWidget(self.base_url_edit, 0, 1, 1, 3)
        grid.addWidget(QtWidgets.QLabel("API Token:"), 0, 4)
        self.token_edit = QtWidgets.QLineEdit(DEFAULT_TOKEN)
        self.token_edit.setEchoMode(QtWidgets.QLineEdit.Password)
        self.token_edit.setToolTip("API token used to authenticate with the DefectDojo API. Keep it secret.")
        grid.addWidget(self.token_edit, 0, 5, 1, 2)

        # Options
        hopt = QtWidgets.QHBoxLayout()
        v.addLayout(hopt)
        self.reuse_chk = QtWidgets.QCheckBox("Reuse existing objects (match by name/username)")
        self.reuse_chk.setToolTip("If enabled, objects found on the target server that match by unique key (e.g. name or username) will be reused and not re-created. Note: mapping to existing objects happens regardless, but enabling this avoids attempts to create duplicates.")
        self.update_chk = QtWidgets.QCheckBox("Update existing objects if found (PUT)")
        self.update_chk.setToolTip("If enabled, objects found on the target server will be updated (PUT) using backup data instead of creating duplicates.")
        self.dryrun_chk = QtWidgets.QCheckBox("Dry-run (no changes)")
        self.dryrun_chk.setToolTip("When enabled, the tool will simulate restore actions and show mappings without performing POST/PUT or file uploads.")
        hopt.addWidget(self.reuse_chk)
        hopt.addWidget(self.update_chk)
        hopt.addWidget(self.dryrun_chk)
        hopt.addStretch()

        # Retry & concurrency
        hr = QtWidgets.QHBoxLayout()
        v.addLayout(hr)
        hr.addWidget(QtWidgets.QLabel("Max retries:"))
        self.retries_spin = QtWidgets.QSpinBox()
        self.retries_spin.setMinimum(0)
        self.retries_spin.setMaximum(10)
        self.retries_spin.setValue(3)
        self.retries_spin.setToolTip("Maximum number of retry attempts for transient network errors on each request. Higher values increase resilience but slow operations.")
        hr.addWidget(self.retries_spin)
        hr.addWidget(QtWidgets.QLabel("Backoff factor (s):"))
        self.backoff_spin = QtWidgets.QDoubleSpinBox()
        self.backoff_spin.setDecimals(2)
        self.backoff_spin.setRange(0.0, 10.0)
        self.backoff_spin.setValue(0.5)
        self.backoff_spin.setToolTip("Base backoff in seconds used for exponential backoff between retries. Wait = backoff * (2 ** attempt).")
        hr.addWidget(self.backoff_spin)
        hr.addWidget(QtWidgets.QLabel("Concurrency:"))
        self.concurrency_spin = QtWidgets.QSpinBox()
        self.concurrency_spin.setMinimum(1)
        self.concurrency_spin.setMaximum(20)
        self.concurrency_spin.setValue(4)
        self.concurrency_spin.setToolTip("Number of worker threads used for concurrent POST/PUT/upload operations per resource. Increase for faster restores on fast networks; reduce if your server rate-limits.")
        hr.addWidget(self.concurrency_spin)
        hr.addStretch()

        # Backup folder + control buttons
        hb = QtWidgets.QHBoxLayout()
        v.addLayout(hb)
        hb.addWidget(QtWidgets.QLabel("Backup folder:"))
        self.backup_folder_edit = QtWidgets.QLineEdit(os.path.expanduser("~"))
        self.backup_folder_edit.setToolTip("Folder where JSON backups and downloaded documents will be saved. Documents are stored under <folder>/documents/<resource>/<orig_id>/*.")
        hb.addWidget(self.backup_folder_edit)
        btn_choose = QtWidgets.QPushButton("Choose...")
        btn_choose.setToolTip("Open folder chooser to select backup folder.")
        btn_choose.clicked.connect(self.choose_backup_folder)
        hb.addWidget(btn_choose)
        self.backup_btn = QtWidgets.QPushButton("Backup Selected")
        self.backup_btn.setToolTip("Start backup for the selected resources. You will be prompted to confirm before backup begins.")
        self.backup_btn.clicked.connect(self.confirm_and_start_backup)
        hb.addWidget(self.backup_btn)
        self.restore_btn = QtWidgets.QPushButton("Restore Files...")
        self.restore_btn.setToolTip("Select backup JSON files and start the restore process. You will be prompted to confirm before restore begins.")
        self.restore_btn.clicked.connect(self.confirm_and_choose_restore_files)
        hb.addWidget(self.restore_btn)
        # Restore from ZIP button
        self.restore_zip_btn = QtWidgets.QPushButton("Restore from ZIP...")
        self.restore_zip_btn.setToolTip("Select a ZIP archive created by this tool, extract it, and restore its JSON + documents contents.")
        self.restore_zip_btn.clicked.connect(self.confirm_and_choose_restore_zip)
        hb.addWidget(self.restore_zip_btn)
        # Load/Export mappings and preflight
        self.load_map_btn = QtWidgets.QPushButton("Load Mappings...")
        self.load_map_btn.setToolTip("Load a previous dd_id_mappings.json to pre-populate mappings before a restore (useful for iterative restores).")
        self.load_map_btn.clicked.connect(self.load_mappings)
        hb.addWidget(self.load_map_btn)
        self.export_map_btn = QtWidgets.QPushButton("Export Last Mappings...")
        self.export_map_btn.setToolTip("Export the last dd_id_mappings.json produced by a restore to a chosen path.")
        self.export_map_btn.clicked.connect(self.export_last_mappings)
        hb.addWidget(self.export_map_btn)
        self.preflight_btn = QtWidgets.QPushButton("Run Preflight")
        self.preflight_btn.setToolTip("Run a preflight analysis of the selected JSON files against the target server to see which objects will be reused/updated/created and unresolved FK references.")
        self.preflight_btn.clicked.connect(self.preflight_dialog)
        hb.addWidget(self.preflight_btn)
        hb.addStretch()

        # Resource checkboxes
        res_h = QtWidgets.QHBoxLayout()
        v.addLayout(res_h)
        self.resource_checks: Dict[str, QtWidgets.QCheckBox] = {}
        for r in RESOURCE_ENDPOINTS.keys():
            cb = QtWidgets.QCheckBox(r.capitalize())
            cb.setChecked(True)
            cb.setToolTip(f"Include {r} in backup/restore operations. Uncheck to skip.")
            self.resource_checks[r] = cb
            res_h.addWidget(cb)
        res_h.addStretch()

        # Progress bars
        v.addWidget(QtWidgets.QLabel("Global progress:"))
        self.global_progress = QtWidgets.QProgressBar()
        self.global_progress.setToolTip("Overall progress across all selected resources (0-100%).")
        v.addWidget(self.global_progress)
        v.addWidget(QtWidgets.QLabel("Resource progress:"))
        self.resource_progress = QtWidgets.QProgressBar()
        self.resource_progress.setToolTip("Progress of the current resource being processed (0-100%).")
        v.addWidget(self.resource_progress)

        # Log area
        v.addWidget(QtWidgets.QLabel("Log:"))
        self.log_edit = QtWidgets.QPlainTextEdit()
        self.log_edit.setReadOnly(True)
        self.log_edit.setFont(QtGui.QFont("Consolas", 10))
        self.log_edit.setToolTip("Operation log. Shows details of API calls, downloads, uploads, errors, and ID mapping summaries.")
        v.addWidget(self.log_edit, 1)

        # Stop button
        hb2 = QtWidgets.QHBoxLayout()
        v.addLayout(hb2)
        self.stop_btn = QtWidgets.QPushButton("Stop")
        self.stop_btn.setEnabled(False)
        self.stop_btn.setToolTip("Stop the running backup or restore operation (best-effort). Operations already in-flight may still complete.")
        self.stop_btn.clicked.connect(self.stop_worker)
        hb2.addWidget(self.stop_btn)
        hb2.addStretch()

    def choose_backup_folder(self):
        folder = QtWidgets.QFileDialog.getExistingDirectory(self, "Select Backup Folder", self.backup_folder_edit.text())
        if folder:
            self.backup_folder_edit.setText(folder)

    @QtCore.pyqtSlot(object)
    def append_log(self, text_obj: object):
        """Slot that receives log messages from worker (queued)."""
        try:
            text = str(text_obj)
        except Exception:
            text = "[{0}] (unprintable log)".format(now_ts())
        # appendPlainText is safe to call from main thread
        self.log_edit.appendPlainText(text)

    def set_progress(self, g: Optional[int], r: Optional[int]):
        if g is not None:
            self.global_progress.setValue(max(0, min(100, int(g))))
        if r is not None:
            self.resource_progress.setValue(max(0, min(100, int(r))))

    # ---------------- Confirmation + Backup startup ----------------
    def confirm_and_start_backup(self):
        selected = [r for r, cb in self.resource_checks.items() if cb.isChecked()]
        if not selected:
            QtWidgets.QMessageBox.warning(self, "No resources", "Select at least one resource to backup.")
            return
        folder = self.backup_folder_edit.text().strip() or os.path.expanduser("~")
        msg = f"Start backup of {len(selected)} resource(s) to folder:\n{folder}\n\nProceed?"
        reply = QtWidgets.QMessageBox.question(self, "Confirm Backup", msg, QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
        if reply == QtWidgets.QMessageBox.Yes:
            self.start_backup()
        else:
            self.append_log(f"[{now_ts()}] Backup cancelled by user.")

    def start_backup(self):
        selected = [r for r, cb in self.resource_checks.items() if cb.isChecked()]
        folder = self.backup_folder_edit.text().strip() or os.path.expanduser("~")
        token = self.token_edit.text().strip()
        base_url = self.base_url_edit.text().strip().rstrip("/")
        t = threading.Thread(target=self._backup_worker, args=(base_url, token, folder, selected), daemon=True)
        t.start()

    def _backup_worker(self, base_url: str, token: str, folder: str, resources: List[str]):
        try:
            self.append_log(f"[{now_ts()}] Starting backup to: {folder}")
            session = requests.Session()
            headers = {"Authorization": f"Token {token}", "Accept": "application/json"}
            total = len(resources)
            step = 0
            docs_root = os.path.join(folder)  # will use folder/documents/...
            for res in resources:
                self.append_log(f"[{now_ts()}] Backing up resource: {res}")
                endpoint = RESOURCE_ENDPOINTS.get(res)
                if not endpoint:
                    self.append_log(f"No endpoint for {res}, skipping.")
                    step += 1
                    self.set_progress(int(step/total*100), 0)
                    continue
                url = base_url + endpoint
                items = []
                cur = url
                while cur:
                    try:
                        r = retry_request(session, "get", cur, headers=headers, max_retries=int(self.retries_spin.value()), backoff_factor=float(self.backoff_spin.value()))
                    except Exception as e:
                        self.append_log(f" GET error: {e}")
                        break
                    if r.status_code != 200:
                        self.append_log(f" GET {cur} -> {r.status_code}: {r.text[:200]}")
                        break
                    data = r.json()
                    if isinstance(data, dict) and "results" in data:
                        items.extend(data.get("results", []))
                        cur = data.get("next")
                    elif isinstance(data, list):
                        items.extend(data)
                        cur = None
                    else:
                        items.append(data)
                        cur = None
                fname = os.path.join(folder, f"{res}.json")
                try:
                    with open(fname, "w", encoding="utf-8") as fh:
                        json.dump(items, fh, indent=2, ensure_ascii=False)
                    self.append_log(f"Wrote {len(items)} objects to {fname}")
                except Exception as e:
                    self.append_log(f"Write error for {fname}: {e}")
                # For certain resources (engagements, tests, findings) download attachments into documents folder
                if res in ("engagements", "tests", "findings"):
                    self.append_log(f"Scanning attachments for resource {res} and downloading into documents folder...")
                    for obj in items:
                        try:
                            orig_id = obj.get("id", "noid")
                            dest_dir = os.path.join(folder, "documents", res, str(orig_id))
                            os.makedirs(dest_dir, exist_ok=True)
                            # 1) Try resource-specific files API
                            api_results = []
                            try:
                                api_results = self._download_files_via_api_local(session, res, orig_id, dest_dir, base_url, token)
                            except Exception as e:
                                self.append_log(f"Files API check failed for {res} id={orig_id}: {e}")
                            if api_results:
                                # already downloaded via API
                                continue
                            # 2) Fallback: scan object fields for URLs
                            def scan_for_urls(value):
                                if isinstance(value, dict):
                                    for kk, vv in value.items():
                                        scan_for_urls(vv)
                                elif isinstance(value, list):
                                    for it in value:
                                        scan_for_urls(it)
                                else:
                                    if isinstance(value, str):
                                        s = value.strip()
                                        if s.startswith("http://") or s.startswith("https://"):
                                            url_val = s
                                            fname_dl = filename_from_url(url_val)
                                            out_path = os.path.join(dest_dir, fname_dl)
                                            try:
                                                rr = retry_request(session, "get", url_val, headers=None, max_retries=int(self.retries_spin.value()), backoff_factor=float(self.backoff_spin.value()))
                                                if rr.status_code == 200:
                                                    with open(out_path, "wb") as fh:
                                                        fh.write(rr.content)
                                                    self.append_log(f"Downloaded {fname_dl} for {res} id={orig_id}")
                                                else:
                                                    self.append_log(f"Failed to download {url_val}: HTTP {rr.status_code}")
                                            except Exception as ee:
                                                self.append_log(f"Exception downloading {url_val}: {ee}")
                            found_any = False
                            for cand in POSSIBLE_ATTACHMENT_KEYS:
                                if cand in obj:
                                    scan_for_urls(obj[cand])
                                    found_any = True
                            if not found_any:
                                scan_for_urls(obj)
                        except Exception as e:
                            self.append_log(f"Error scanning attachments for object id {obj.get('id')}: {e}")
                step += 1
                self.set_progress(int(step/total*100), 0)
            self.append_log(f"[{now_ts()}] Backup finished.")
            self.set_progress(100, 100)
        except Exception as e:
            self.append_log(f"[{now_ts()}] Fatal backup error: {e}")
            self.append_log(traceback.format_exc())

    def _download_files_via_api_local(self, session: requests.Session, resource: str, orig_id: Any, dest_dir: str, base_url: str, token: str) -> List[Dict[str, Any]]:
        """
        Local helper used in backup flow to call resource-specific files list & download endpoints.
        Mirrors RestoreWorker.download_files_via_api but simpler to avoid coupling.
        """
        results: List[Dict[str, Any]] = []
        try:
            if resource == "engagements":
                list_ep = FILES_ENDPOINTS.get("engagements_list")
                dl_ep = FILES_ENDPOINTS.get("engagements_download")
            elif resource == "tests":
                list_ep = FILES_ENDPOINTS.get("tests_list")
                dl_ep = FILES_ENDPOINTS.get("tests_download")
            elif resource == "findings":
                list_ep = FILES_ENDPOINTS.get("findings_list")
                dl_ep = FILES_ENDPOINTS.get("findings_download")
            else:
                return results
            if not list_ep or not dl_ep:
                return results
            list_url = base_url + list_ep.format(id=orig_id)
            headers = {"Authorization": f"Token {token}", "Accept": "application/json"}
            r = retry_request(session, "get", list_url, headers=headers, max_retries=int(self.retries_spin.value()), backoff_factor=float(self.backoff_spin.value()))
            if r.status_code != 200:
                return results
            data = r.json()
            files_list = []
            if isinstance(data, dict) and "files" in data:
                files_list = data.get("files", [])
            elif isinstance(data, list):
                files_list = data
            else:
                files_list = data if isinstance(data, list) else []
            os.makedirs(dest_dir, exist_ok=True)
            for fmeta in files_list:
                try:
                    file_id = fmeta.get("id")
                    remote_name = None
                    if "file" in fmeta and isinstance(fmeta.get("file"), str):
                        remote_name = os.path.basename(urlparse(fmeta.get("file")).path) if fmeta.get("file") else None
                    dl_url = base_url + dl_ep.format(id=orig_id, file_id=file_id)
                    rr = retry_request(session, "get", dl_url, headers={"Authorization": f"Token {token}"}, max_retries=int(self.retries_spin.value()), backoff_factor=float(self.backoff_spin.value()))
                    if rr.status_code == 200:
                        fname = remote_name or f"{resource}_{orig_id}_{file_id}"
                        cd = rr.headers.get("Content-Disposition")
                        if cd and "filename=" in cd:
                            try:
                                fname = cd.split("filename=")[1].strip().strip('";')
                            except Exception:
                                pass
                        out_path = os.path.join(dest_dir, fname)
                        with open(out_path, "wb") as fh:
                            fh.write(rr.content)
                        content_type = rr.headers.get("Content-Type", guess_mime(fname))
                        results.append({"filename": fname, "path": out_path, "file_id": file_id, "content_type": content_type, "field": "files"})
                        self.append_log(f"Downloaded via files API: {resource} id={orig_id} file_id={file_id} -> {fname}")
                    else:
                        self.append_log(f"Failed to download file via API {dl_url}: HTTP {rr.status_code}")
                except Exception as e:
                    self.append_log(f"Exception downloading file metadata {fmeta}: {e}")
            return results
        except Exception as e:
            self.append_log(f"Files API download error for {resource} id={orig_id}: {e}")
            return results

    # ---------------- Confirmation + Restore startup ----------------
    def confirm_and_choose_restore_files(self):
        # Ask user to select backup files
        files, _ = QtWidgets.QFileDialog.getOpenFileNames(self, "Select JSON files to restore", os.path.expanduser("~"), "JSON Files (*.json);;All Files (*)")
        if not files:
            return
        base_url = self.base_url_edit.text().strip()
        token = self.token_edit.text().strip()
        options = {
            "reuse_existing": self.reuse_chk.isChecked(),
            "update_if_exists": self.update_chk.isChecked(),
            "dry_run": self.dryrun_chk.isChecked(),
            "max_retries": int(self.retries_spin.value()),
            "backoff_factor": float(self.backoff_spin.value()),
            "concurrency": int(self.concurrency_spin.value())
        }
        folder = self.backup_folder_edit.text().strip() or None
        # confirm popup
        msg = f"Start restore of {len(files)} file(s) into server:\n{base_url}\n\nOptions:\n  Reuse existing: {options['reuse_existing']}\n  Update if exists: {options['update_if_exists']}\n  Dry-run: {options['dry_run']}\n\nProceed?"
        reply = QtWidgets.QMessageBox.question(self, "Confirm Restore", msg, QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
        if reply == QtWidgets.QMessageBox.Yes:
            self.start_restore(base_url, token, files, options, folder, initial_mappings=self._loaded_mappings)
        else:
            self.append_log(f"[{now_ts()}] Restore cancelled by user.")

    def confirm_and_choose_restore_zip(self):
        # New: choose zip file containing backup (JSON files & documents)
        zf_path, _ = QtWidgets.QFileDialog.getOpenFileName(self, "Select Backup ZIP to restore", os.path.expanduser("~"), "ZIP Archives (*.zip);;All Files (*)")
        if not zf_path:
            return
        if not os.path.isfile(zf_path):
            QtWidgets.QMessageBox.warning(self, "File missing", "Selected ZIP file does not exist.")
            return
        # Ask user for confirmation
        base_url = self.base_url_edit.text().strip()
        token = self.token_edit.text().strip()
        options = {
            "reuse_existing": self.reuse_chk.isChecked(),
            "update_if_exists": self.update_chk.isChecked(),
            "dry_run": self.dryrun_chk.isChecked(),
            "max_retries": int(self.retries_spin.value()),
            "backoff_factor": float(self.backoff_spin.value()),
            "concurrency": int(self.concurrency_spin.value())
        }
        msg = f"Restore from ZIP:\n{zf_path}\n\nTarget server: {base_url}\nOptions:\n  Reuse existing: {options['reuse_existing']}\n  Update if exists: {options['update_if_exists']}\n  Dry-run: {options['dry_run']}\n\nProceed?"
        reply = QtWidgets.QMessageBox.question(self, "Confirm Restore from ZIP", msg, QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
        if reply != QtWidgets.QMessageBox.Yes:
            self.append_log(f"[{now_ts()}] Restore-from-ZIP cancelled by user.")
            return

        # Extract ZIP to a temporary directory and find JSON files
        try:
            temp_dir = tempfile.mkdtemp(prefix="dd_restore_")
            self._temp_restore_dir = temp_dir  # remember to cleanup later
            self.append_log(f"[{now_ts()}] Extracting ZIP to temporary folder: {temp_dir}")
            with zipfile.ZipFile(zf_path, 'r') as zf:
                zf.extractall(temp_dir)
            # discover JSON files at root or within top-level folder
            candidate_jsons: List[str] = []
            # walk the extracted tree and collect files that look like the resource jsons: name.json where name in RESOURCE_ENDPOINTS
            for root, dirs, files in os.walk(temp_dir):
                for f in files:
                    lower = f.lower()
                    name_noext = os.path.splitext(lower)[0]
                    if lower.endswith(".json") and name_noext in RESOURCE_ENDPOINTS.keys():
                        candidate_jsons.append(os.path.join(root, f))
            if not candidate_jsons:
                # try fallback: any .json files at top level
                for f in os.listdir(temp_dir):
                    if f.lower().endswith(".json"):
                        candidate_jsons.append(os.path.join(temp_dir, f))
            if not candidate_jsons:
                QtWidgets.QMessageBox.warning(self, "No JSON found", "No resource JSON files were found inside the ZIP. Make sure the ZIP contains <resource>.json files.")
                # cleanup temp_dir
                try:
                    shutil.rmtree(temp_dir)
                    self._temp_restore_dir = None
                except Exception:
                    pass
                return
            # start restore using extracted json files and using the extracted documents folder as backup_docs_root
            backup_docs_root = temp_dir  # the worker will look for temp_dir/documents/<resource>/<id>/
            self.append_log(f"[{now_ts()}] Found {len(candidate_jsons)} JSON resource file(s) inside ZIP. Starting restore...")
            self.start_restore(base_url, token, candidate_jsons, options, backup_docs_root, initial_mappings=self._loaded_mappings)
        except Exception as e:
            self.append_log(f"[{now_ts()}] Failed to extract or start restore from ZIP: {e}")
            if self._temp_restore_dir:
                try:
                    shutil.rmtree(self._temp_restore_dir)
                except Exception:
                    pass
                self._temp_restore_dir = None

    def start_restore(self, base_url: str, token: str, files: List[str], options: Dict[str, Any], backup_docs_root: Optional[str], initial_mappings: Optional[Dict[str, Any]] = None):
        # disable UI controls
        self.backup_btn.setEnabled(False)
        self.restore_btn.setEnabled(False)
        self.restore_zip_btn.setEnabled(False)
        self.preflight_btn.setEnabled(False)
        self.load_map_btn.setEnabled(False)
        self.export_map_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)
        self.append_log(f"[{now_ts()}] Starting restore...")

        # Create QThread and Move worker to it. Create the worker in the main thread then move it — that is safe.
        self.worker_thread = QtCore.QThread()
        self.worker_obj = RestoreWorker(base_url, token, files, options, backup_docs_root=backup_docs_root, initial_mappings=initial_mappings)
        # move worker to the thread
        self.worker_obj.moveToThread(self.worker_thread)

        # connect signals using QueuedConnection to ensure GUI-thread execution of slots
        self.worker_obj.log_signal.connect(self.append_log, QtCore.Qt.QueuedConnection)
        self.worker_obj.progress_signal.connect(lambda g, r: self.set_progress(g, r), QtCore.Qt.QueuedConnection)
        self.worker_obj.done_signal.connect(self.restore_done, QtCore.Qt.QueuedConnection)
        self.worker_obj.error_signal.connect(self.restore_error, QtCore.Qt.QueuedConnection)

        # capture mapping persistence from worker by watching log for saved path (also read file after done)
        def capture_mappings(msg_obj):
            try:
                msg = str(msg_obj)
            except Exception:
                return
            if "Saved ID mappings to " in msg:
                try:
                    path = msg.split("Saved ID mappings to ",1)[1].strip()
                    if os.path.isfile(path):
                        try:
                            with open(path, "r", encoding="utf-8") as fh:
                                self._last_mappings = json.load(fh)
                        except Exception:
                            self._last_mappings = None
                except Exception:
                    pass
        self.worker_obj.log_signal.connect(capture_mappings, QtCore.Qt.QueuedConnection)

        # start the worker when thread starts
        self.worker_thread.started.connect(self.worker_obj.run, QtCore.Qt.QueuedConnection)

        # Ensure we delete the worker object only from the main thread after the thread finishes
        def thread_finished_cleanup():
            try:
                if self.worker_obj is not None:
                    # schedule deletion of worker_obj on main thread
                    self.worker_obj.deleteLater()
            except Exception:
                pass

        self.worker_thread.finished.connect(thread_finished_cleanup, QtCore.Qt.QueuedConnection)

        # Start thread
        self.worker_thread.start()

    def stop_worker(self):
        if self.worker_obj:
            # request the worker to stop (it's thread-safe flag)
            self.worker_obj.stop()
            self.append_log(f"[{now_ts()}] Stop requested...")
        # also ask thread to quit after worker honors stop
        if self.worker_thread:
            try:
                # let the worker cleanly finish; do not forcefully terminate UI objects.
                self.worker_thread.quit()
            except Exception:
                pass

    def restore_done(self, message: object):
        try:
            msg = str(message)
        except Exception:
            msg = "[done]"
        self.append_log(f"[{now_ts()}] {msg}")
        # try to reload last mappings if produced
        if self._last_mappings:
            try:
                # show a short confirmation in log
                self.append_log(f"[{now_ts()}] Last mappings loaded into memory for Export.")
            except Exception:
                pass
        self._finish_restore_cleanup()

    def restore_error(self, message: object):
        try:
            msg = str(message)
        except Exception:
            msg = "[error]"
        self.append_log(f"[{now_ts()}] ERROR: {msg}")
        self._finish_restore_cleanup()

    def _finish_restore_cleanup(self):
        self.backup_btn.setEnabled(True)
        self.restore_btn.setEnabled(True)
        self.restore_zip_btn.setEnabled(True)
        self.preflight_btn.setEnabled(True)
        self.load_map_btn.setEnabled(True)
        self.export_map_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)
        self.set_progress(100, 100)
        # cleanup temp extracted folder if present
        if self._temp_restore_dir:
            try:
                shutil.rmtree(self._temp_restore_dir)
                self.append_log(f"[{now_ts()}] Cleaned up temporary extracted folder: {self._temp_restore_dir}")
            except Exception as e:
                self.append_log(f"[{now_ts()}] Warning: failed to remove temporary folder {self._temp_restore_dir}: {e}")
            self._temp_restore_dir = None
        # Properly stop and delete the thread & worker
        if self.worker_thread:
            try:
                # ask thread to quit (if not already)
                self.worker_thread.quit()
                # wait a short time for it to finish
                self.worker_thread.wait(2000)
            except Exception:
                pass
            try:
                # schedule deletion
                self.worker_thread.deleteLater()
            except Exception:
                pass
            self.worker_thread = None
        # ensure worker_obj reference removed (worker was scheduled for deletion earlier)
        self.worker_obj = None

    # ---------------- Preflight UI and mapping load/export ----------------
    def preflight_dialog(self):
        files, _ = QtWidgets.QFileDialog.getOpenFileNames(self, "Select JSON files to preflight", os.path.expanduser("~"), "JSON Files (*.json);;All Files (*)")
        if not files:
            return
        base_url = self.base_url_edit.text().strip()
        token = self.token_edit.text().strip()
        options = {
            "reuse_existing": self.reuse_chk.isChecked(),
            "update_if_exists": self.update_chk.isChecked(),
            "dry_run": self.dryrun_chk.isChecked(),
            "max_retries": int(self.retries_spin.value()),
            "backoff_factor": float(self.backoff_spin.value()),
            "concurrency": int(self.concurrency_spin.value())
        }
        # run preflight in a worker thread? For simplicity we'll run synchronously but show a busy cursor (network IO occurs)
        QtWidgets.QApplication.setOverrideCursor(QtGui.QCursor(QtCore.Qt.WaitCursor))
        try:
            summary = analyze_preflight(base_url, token, files, options, initial_mappings=self._loaded_mappings)
        finally:
            QtWidgets.QApplication.restoreOverrideCursor()
        # show in a modal dialog with option to save mapping suggestions
        dlg = QtWidgets.QDialog(self)
        dlg.setWindowTitle("Preflight Summary")
        dlg.resize(900, 600)
        layout = QtWidgets.QVBoxLayout(dlg)
        text = QtWidgets.QPlainTextEdit()
        text.setReadOnly(True)
        text.setPlainText(summary)
        layout.addWidget(text)
        btns = QtWidgets.QHBoxLayout()
        layout.addLayout(btns)
        btn_save = QtWidgets.QPushButton("Save Summary...")
        btn_save.clicked.connect(lambda: self._save_preflight_summary(summary))
        btns.addWidget(btn_save)
        btn_ok = QtWidgets.QPushButton("OK")
        btn_ok.clicked.connect(dlg.accept)
        btns.addStretch()
        btns.addWidget(btn_ok)
        dlg.exec_()

    def _save_preflight_summary(self, summary: str):
        path, _ = QtWidgets.QFileDialog.getSaveFileName(self, "Save Preflight Summary", os.path.expanduser("~/preflight_summary.txt"), "Text Files (*.txt);;All Files (*)")
        if not path:
            return
        try:
            with open(path, "w", encoding="utf-8") as fh:
                fh.write(summary)
            QtWidgets.QMessageBox.information(self, "Saved", f"Preflight summary saved to:\n{path}")
        except Exception as e:
            QtWidgets.QMessageBox.warning(self, "Save failed", f"Failed to save: {e}")

    def load_mappings(self):
        path, _ = QtWidgets.QFileDialog.getOpenFileName(self, "Load dd_id_mappings.json", os.path.expanduser("~"), "JSON Files (*.json);;All Files (*)")
        if not path:
            return
        try:
            with open(path, "r", encoding="utf-8") as fh:
                data = json.load(fh)
            self._loaded_mappings = data
            self.append_log(f"[{now_ts()}] Loaded mappings from {path}; they will be used by next restore.")
            QtWidgets.QMessageBox.information(self, "Mappings Loaded", f"Mappings loaded from:\n{path}\nThey will be used in next restore.")
        except Exception as e:
            QtWidgets.QMessageBox.warning(self, "Load failed", f"Failed to load mappings: {e}")

    def export_last_mappings(self):
        if not self._last_mappings:
            QtWidgets.QMessageBox.information(self, "No mappings", "No mappings available to export yet. Run a restore first.")
            return
        path, _ = QtWidgets.QFileDialog.getSaveFileName(self, "Export mappings to", os.path.expanduser("~/dd_id_mappings_export.json"), "JSON Files (*.json);;All Files (*)")
        if not path:
            return
        try:
            with open(path, "w", encoding="utf-8") as fh:
                json.dump(self._last_mappings, fh, indent=2, ensure_ascii=False)
            QtWidgets.QMessageBox.information(self, "Exported", f"Mappings exported to:\n{path}")
        except Exception as e:
            QtWidgets.QMessageBox.warning(self, "Export failed", f"Failed to export mappings: {e}")

# ---------------- Main ----------------
def main():
    app = QtWidgets.QApplication(sys.argv)
    win = MainWindow()
    win.show()
    sys.exit(app.exec_())

if __name__ == "__main__":
    main()