#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
dojo_restore_with_mappings.py

Restores DefectDojo objects in a deterministic sequence with old-id -> new-id mapping,
and supports loading a pre-existing dd_id_mappings.json to seed mappings (iterative restores).

Sequence:
  1) Products  -> writes new_products.json
  2) Users     -> writes new_users.json
  3) Engagements (remap products/users) -> writes new_engagements.json
  4) Tests (remap engagements/products/users) -> writes new_tests.json and uploads attachments
  5) Findings (remap tests/engagements/products/users) -> writes new_findings.json and uploads attachments

Usage:
    python dojo_restore_with_mappings.py --base-url https://demo.defectdojo.org --token <TOKEN> \
        --backup-dir /path/to/backup --mappings /path/to/dd_id_mappings.json

Author: Assistant
Date: 2025-10-04
"""
from __future__ import annotations
import argparse
import os
import sys
import json
import time
import threading
import traceback
from typing import Any, Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import mimetypes

# ---------------- Defaults & Endpoints ----------------
DEFAULT_BASE_URL = "https://demo.defectdojo.org"

RESOURCE_ENDPOINTS = {
    "products": "/api/v2/products/",
    "users": "/api/v2/users/",
    "engagements": "/api/v2/engagements/",
    "tests": "/api/v2/tests/",
    "findings": "/api/v2/findings/",
}

FILES_ENDPOINTS = {
    "engagements_files": "/api/v2/engagements/{id}/files/",
    "engagements_files_download": "/api/v2/engagements/{id}/files/download/{file_id}/",
    "tests_files": "/api/v2/tests/{id}/files/",
    "tests_files_download": "/api/v2/tests/{id}/files/download/{file_id}/",
    "findings_files": "/api/v2/findings/{id}/files/",
    "findings_files_download": "/api/v2/findings/{id}/files/download/{file_id}/",
}

DOCUMENTS_ENDPOINT = "/api/v2/documents/"

# Server-managed fields to strip before POST/PUT
SERVER_FIELDS_TO_STRIP = {"id", "created", "updated", "date", "last_modified", "active", "deleted", "url", "uuid"}

# ---------------- Utilities ----------------
def now_ts() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")

def strip_server_fields(obj: Any) -> Any:
    """Recursively strip server-managed fields from JSON-like object."""
    if isinstance(obj, dict):
        out = {}
        for k, v in obj.items():
            if k.lower() in SERVER_FIELDS_TO_STRIP:
                continue
            out[k] = strip_server_fields(v)
        return out
    if isinstance(obj, list):
        return [strip_server_fields(i) for i in obj]
    return obj

def guess_mime(fname: str) -> str:
    return mimetypes.guess_type(fname)[0] or "application/octet-stream"

# Exponential backoff HTTP wrapper
def retry_request(session: requests.Session, method: str, url: str, headers: Optional[Dict[str,str]] = None,
                  json_payload: Optional[Any] = None, data: Optional[Dict[str,str]] = None,
                  files: Optional[Dict[str,Tuple[str,Any,str]]] = None,
                  timeout: int = 60, max_retries: int = 3, backoff_factor: float = 0.5) -> requests.Response:
    attempt = 0
    last_exc = None
    while attempt <= max_retries:
        try:
            if method.lower() == "get":
                r = session.get(url, headers=headers, timeout=timeout)
            elif method.lower() == "post":
                if files is not None:
                    r = session.post(url, headers=headers, files=files, data=data, timeout=timeout)
                elif json_payload is not None:
                    r = session.post(url, headers=headers, json=json_payload, timeout=timeout)
                else:
                    r = session.post(url, headers=headers, data=data, timeout=timeout)
            elif method.lower() == "put":
                if json_payload is not None:
                    r = session.put(url, headers=headers, json=json_payload, timeout=timeout)
                else:
                    r = session.put(url, headers=headers, data=data, timeout=timeout)
            else:
                raise ValueError("Unsupported HTTP method: " + method)
            return r
        except Exception as e:
            last_exc = e
            wait = backoff_factor * (2 ** attempt)
            time.sleep(wait)
            attempt += 1
    raise last_exc

# ---------------- Core Restorer ----------------
class DojoRestorer:
    def __init__(self, base_url: str, token: str, backup_dir: str,
                 concurrency: int = 2, max_retries: int = 3, backoff_factor: float = 0.5,
                 verbose: bool = True, logfile: Optional[str] = None):
        self.base_url = base_url.rstrip("/")
        self.token = token
        self.backup_dir = os.path.abspath(backup_dir)
        self.concurrency = max(1, int(concurrency))
        self.max_retries = int(max_retries)
        self.backoff_factor = float(backoff_factor)
        self.verbose = verbose
        self.logfile = logfile

        self.session = requests.Session()
        self.session.headers.update({"Authorization": f"Token {self.token}", "Accept": "application/json"})

        self.lock = threading.Lock()
        # mapping_store[resource][old_id] = new_id
        self.mapping_store: Dict[str, Dict[int, Optional[int]]] = {k: {} for k in RESOURCE_ENDPOINTS.keys()}
        # name_to_newid for products/users as fallback (name->new_id)
        self.name_to_newid: Dict[str, Dict[str, int]] = {"products": {}, "users": {}}

    def log(self, *parts):
        line = f"[{now_ts()}] " + " ".join(str(p) for p in parts)
        if self.verbose:
            print(line)
        if self.logfile:
            try:
                with open(self.logfile, "a", encoding="utf-8") as fh:
                    fh.write(line + "\n")
            except Exception:
                pass

    def load_mappings(self, mappings_path: str):
        """Load dd_id_mappings.json to seed mapping_store and name_to_newid."""
        if not mappings_path or not os.path.isfile(mappings_path):
            self.log(f"Mappings file not found: {mappings_path}")
            return
        try:
            with open(mappings_path, "r", encoding="utf-8") as fh:
                data = json.load(fh)
        except Exception as e:
            self.log("Failed to read mappings file:", e)
            return
        ms = data.get("mapping_store") or data.get("mappings") or {}
        for res, d in ms.items():
            if res in self.mapping_store and isinstance(d, dict):
                for oldk, newv in d.items():
                    try:
                        oldi = int(oldk)
                    except Exception:
                        continue
                    # newv may be None or int
                    self.mapping_store[res][oldi] = int(newv) if isinstance(newv, int) else newv
        nt = data.get("name_to_newid") or {}
        for res, d in nt.items():
            if isinstance(d, dict):
                for name, nid in d.items():
                    try:
                        self.name_to_newid.setdefault(res, {})[str(name)] = int(nid)
                    except Exception:
                        continue
        self.log(f"Loaded mappings from {mappings_path}. Seeded mapping_store and name_to_newid.")

    def persist_mappings(self):
        out = {
            "mapping_store": self.mapping_store,
            "name_to_newid": self.name_to_newid
        }
        path = os.path.join(self.backup_dir, "dd_id_mappings.json")
        try:
            with open(path, "w", encoding="utf-8") as fh:
                json.dump(out, fh, indent=2, ensure_ascii=False)
            self.log(f"Saved mappings to {path}")
        except Exception as e:
            self.log("Failed to save mappings:", e)

    def _load_json(self, fname: str) -> List[Dict[str,Any]]:
        path = os.path.join(self.backup_dir, fname)
        if not os.path.isfile(path):
            self.log(f"Warning: {fname} not found in backup dir {self.backup_dir}.")
            return []
        try:
            with open(path, "r", encoding="utf-8") as fh:
                data = json.load(fh)
        except Exception as e:
            self.log(f"Failed to load {path}: {e}")
            return []
        if isinstance(data, list):
            return data
        if isinstance(data, dict):
            return [data]
        return []

    def _write_json(self, fname: str, data: List[Dict[str,Any]]):
        path = os.path.join(self.backup_dir, fname)
        try:
            with open(path, "w", encoding="utf-8") as fh:
                json.dump(data, fh, indent=2, ensure_ascii=False)
            self.log(f"Wrote {len(data)} objects to {path}")
        except Exception as e:
            self.log(f"Failed to write {path}: {e}")

    # ---------------- Remap logic ----------------
    def remap_object_refs(self, obj: Any) -> Any:
        """
        Recursively remap references using mapping_store.
        If numeric old_id mapping missing for products/users tries name->newid fallback (if backup included name).
        """
        if isinstance(obj, dict):
            out = {}
            for k, v in obj.items():
                # fields like product_id
                if k.endswith("_id") and isinstance(v, int):
                    prefix = k[:-3].lower()
                    candidate = prefix + "s" if prefix + "s" in self.mapping_store else prefix
                    mapped = None
                    if candidate in self.mapping_store:
                        mapped = self.mapping_store[candidate].get(v)
                    if mapped is None and candidate in ("products", "users"):
                        # fallback not possible without backup name; caller should populate name_to_newid earlier
                        mapped = self.mapping_store.get(candidate, {}).get(v)
                    out[k] = mapped if mapped is not None else v
                elif k.lower() in ("product", "engagement", "test", "user", "owner"):
                    # value could be int id or nested object
                    if isinstance(v, int):
                        candidate = None
                        if k.lower() in ("user", "owner"):
                            candidate = "users"
                        elif k.lower() == "product":
                            candidate = "products"
                        elif k.lower() == "engagement":
                            candidate = "engagements"
                        elif k.lower() == "test":
                            candidate = "tests"
                        mapped = None
                        if candidate:
                            mapped = self.mapping_store.get(candidate, {}).get(v)
                        out[k] = mapped if mapped is not None else v
                    elif isinstance(v, dict) and "id" in v and isinstance(v["id"], int):
                        inner = v.copy()
                        old = inner["id"]
                        candidate = None
                        if k.lower() in ("user", "owner"):
                            candidate = "users"
                        elif k.lower() == "product":
                            candidate = "products"
                        elif k.lower() == "engagement":
                            candidate = "engagements"
                        elif k.lower() == "test":
                            candidate = "tests"
                        if candidate:
                            mapped = self.mapping_store.get(candidate, {}).get(old)
                            if mapped is not None:
                                inner["id"] = mapped
                        out[k] = self.remap_object_refs(inner)
                    else:
                        out[k] = self.remap_object_refs(v)
                else:
                    out[k] = self.remap_object_refs(v)
            return out
        if isinstance(obj, list):
            return [self.remap_object_refs(i) for i in obj]
        return obj

    # ---------------- Resource creation helpers ----------------
    def _create_objects_concurrent(self, resource: str, objects: List[Dict[str,Any]]) -> List[Dict[str,Any]]:
        """
        Create objects via POST to /api/v2/<resource>/ with concurrency.
        Returns list of successful response objects.
        """
        if not objects:
            return []
        url = self.base_url + RESOURCE_ENDPOINTS[resource]
        created = []
        self.log(f"Creating {len(objects)} {resource} (concurrency={self.concurrency})")

        def worker(obj):
            old_id = obj.get("id")
            payload = strip_server_fields(obj)
            try:
                r = retry_request(self.session, "post", url, headers=None, json_payload=payload,
                                  max_retries=self.max_retries, backoff_factor=self.backoff_factor)
                if r is not None and r.status_code in (200,201):
                    try:
                        j = r.json()
                    except Exception:
                        j = {}
                    new_id = j.get("id")
                    with self.lock:
                        if old_id is not None:
                            self.mapping_store[resource][int(old_id)] = int(new_id) if new_id is not None else None
                        # record name->newid for products/users
                        if resource == "products":
                            name = j.get("name") or payload.get("name")
                            if name and new_id:
                                self.name_to_newid.setdefault("products", {})[str(name)] = int(new_id)
                        if resource == "users":
                            username = j.get("username") or payload.get("username")
                            if username and new_id:
                                self.name_to_newid.setdefault("users", {})[str(username)] = int(new_id)
                    self.log(f"Created {resource[:-1]} old_id={old_id} -> new_id={new_id}")
                    return j
                else:
                    self.log(f"Failed to create {resource[:-1]} old_id={old_id}: status={getattr(r,'status_code',None)}")
                    return {"__failed__": True, "old_id": old_id, "status": getattr(r,'status_code',None), "body": getattr(r,'text','')}
            except Exception as e:
                self.log(f"Exception creating {resource[:-1]} old_id={old_id}: {e}")
                return {"__failed__": True, "old_id": old_id, "exception": str(e)}

        results = []
        if self.concurrency > 1:
            with ThreadPoolExecutor(max_workers=self.concurrency) as exe:
                futures = {exe.submit(worker, obj): obj for obj in objects}
                for fut in as_completed(futures):
                    results.append(fut.result())
        else:
            for obj in objects:
                results.append(worker(obj))

        created = [r for r in results if isinstance(r, dict) and not r.get("__failed__")]
        return created

    # ---------------- Step 1: Products ----------------
    def create_products(self, input_fname: str = "products.json", output_fname: str = "new_products.json"):
        prods = self._load_json(input_fname)
        if not prods:
            self.log("No products.json found or file empty.")
            return []
        created = self._create_objects_concurrent("products", prods)
        self._write_json(output_fname, created)
        self.persist_mappings()
        return created

    # ---------------- Step 2: Users ----------------
    def create_users(self, input_fname: str = "users.json", output_fname: str = "new_users.json"):
        users = self._load_json(input_fname)
        if not users:
            self.log("No users.json found or file empty.")
            return []
        created = self._create_objects_concurrent("users", users)
        self._write_json(output_fname, created)
        self.persist_mappings()
        return created

    # ---------------- Step 3: Engagements ----------------
    def create_engagements(self, input_fname: str = "engagements.json", output_fname: str = "new_engagements.json"):
        engs = self._load_json(input_fname)
        if not engs:
            self.log("No engagements.json found or file empty.")
            return []
        created = []
        url = self.base_url + RESOURCE_ENDPOINTS["engagements"]
        self.log(f"Creating {len(engs)} engagements (sequential to ensure mapping correctness).")
        for obj in engs:
            old_id = obj.get("id")
            payload = strip_server_fields(obj)
            payload = self.remap_object_refs(payload)
            try:
                r = retry_request(self.session, "post", url, headers=None, json_payload=payload,
                                  max_retries=self.max_retries, backoff_factor=self.backoff_factor)
                if r is not None and r.status_code in (200,201):
                    try:
                        j = r.json()
                    except Exception:
                        j = {}
                    new_id = j.get("id")
                    with self.lock:
                        if old_id is not None:
                            self.mapping_store["engagements"][int(old_id)] = int(new_id) if new_id is not None else None
                    created.append(j)
                    self.log(f"Created engagement old_id={old_id} -> new_id={new_id}")
                else:
                    self.log(f"Failed to create engagement old_id={old_id}: status={getattr(r,'status_code',None)}")
            except Exception as e:
                self.log(f"Exception creating engagement old_id={old_id}: {e}")
        self._write_json(output_fname, created)
        self.persist_mappings()
        return created

    # ---------------- Helper: upload files to resource-specific endpoint or fallback to documents ----------------
    def _upload_file_to_resource(self, resource: str, target_id: int, file_path: str) -> Tuple[str, Optional[int], str]:
        """
        Attempt to POST the file to resource-specific files API.
        If it fails, attempt to POST to /api/v2/documents/ with proper FK field.
        Returns (filename, status_code or None, response_text or exception).
        """
        fname = os.path.basename(file_path)
        # Step A: try resource-specific endpoint
        ep_key = None
        if resource == "tests":
            ep_key = "tests_files"
        elif resource == "findings":
            ep_key = "findings_files"
        elif resource == "engagements":
            ep_key = "engagements_files"
        if ep_key and ep_key in FILES_ENDPOINTS:
            url = self.base_url + FILES_ENDPOINTS[ep_key].format(id=target_id)
            headers = {"Authorization": f"Token {self.token}"}
            fobj = open(file_path, "rb")
            files = {"file": (fname, fobj, guess_mime(fname))}
            data = {"title": fname}
            try:
                r = retry_request(self.session, "post", url, headers=headers, files=files, data=data,
                                  max_retries=self.max_retries, backoff_factor=self.backoff_factor)
                try:
                    fobj.close()
                except Exception:
                    pass
                status = getattr(r, "status_code", None)
                body = getattr(r, "text", "")[:1000]
                if status in (200,201):
                    self.log(f"Uploaded file '{fname}' to {resource} id={target_id} via files endpoint (status {status})")
                    return (fname, status, body)
                else:
                    self.log(f"Files endpoint upload returned status {status} for {fname}; will try documents fallback.")
            except Exception as e:
                try:
                    fobj.close()
                except Exception:
                    pass
                self.log(f"Exception uploading file to files endpoint: {e}; will try documents fallback.")

        # Step B: fallback to documents endpoint
        try:
            url = self.base_url + DOCUMENTS_ENDPOINT
            headers = {"Authorization": f"Token {self.token}"}
            fobj = open(file_path, "rb")
            files = {"file": (fname, fobj, guess_mime(fname))}
            # attach appropriate FK field
            data = {"title": fname}
            if resource == "tests":
                data["test"] = str(target_id)
            elif resource == "findings":
                data["finding"] = str(target_id)
            elif resource == "engagements":
                data["engagement"] = str(target_id)
            try:
                r = retry_request(self.session, "post", url, headers=headers, files=files, data=data,
                                  max_retries=self.max_retries, backoff_factor=self.backoff_factor)
                try:
                    fobj.close()
                except Exception:
                    pass
                status = getattr(r, "status_code", None)
                body = getattr(r, "text", "")[:1000]
                if status in (200,201):
                    self.log(f"Uploaded file '{fname}' to documents endpoint and attached to {resource} id={target_id} (status {status})")
                    return (fname, status, body)
                else:
                    self.log(f"Documents endpoint upload returned status {status} for {fname}")
                    return (fname, status, body)
            except Exception as e:
                try:
                    fobj.close()
                except Exception:
                    pass
                self.log(f"Exception uploading file to documents endpoint: {e}")
                return (fname, None, str(e))
        except Exception as e:
            self.log(f"Unexpected exception for file fallback: {e}")
            return (fname, None, str(e))

    # ---------------- Step 4: Tests ----------------
    def create_tests(self, input_fname: str = "tests.json", output_fname: str = "new_tests.json"):
        tests = self._load_json(input_fname)
        if not tests:
            self.log("No tests.json found or file empty.")
            return []
        created = []
        url = self.base_url + RESOURCE_ENDPOINTS["tests"]
        self.log(f"Creating {len(tests)} tests sequentially (so uploads attach to known new IDs).")
        for obj in tests:
            old_id = obj.get("id")
            payload = strip_server_fields(obj)
            payload = self.remap_object_refs(payload)
            try:
                r = retry_request(self.session, "post", url, headers=None, json_payload=payload,
                                  max_retries=self.max_retries, backoff_factor=self.backoff_factor)
                if r is not None and r.status_code in (200,201):
                    try:
                        j = r.json()
                    except Exception:
                        j = {}
                    new_id = j.get("id")
                    with self.lock:
                        if old_id is not None:
                            self.mapping_store["tests"][int(old_id)] = int(new_id) if new_id is not None else None
                    self.log(f"Created test old_id={old_id} -> new_id={new_id}")
                    # attempt to upload attachments from backup_dir/documents/tests/<old_id>/*
                    docs_folder = os.path.join(self.backup_dir, "documents", "tests", str(old_id))
                    if os.path.isdir(docs_folder):
                        files = [os.path.join(docs_folder, f) for f in os.listdir(docs_folder) if os.path.isfile(os.path.join(docs_folder, f))]
                        self.log(f"Found {len(files)} attachments for test old_id={old_id}; uploading...")
                        for fpath in files:
                            fname, status, resp = self._upload_file_to_resource("tests", new_id, fpath)
                            self.log(f"  upload {fname}: status={status}")
                    else:
                        self.log(f"No attachments folder for test old_id={old_id} at {docs_folder}")
                    created.append(j)
                else:
                    self.log(f"Failed to create test old_id={old_id}: status={getattr(r,'status_code',None)}")
            except Exception as e:
                self.log(f"Exception creating test old_id={old_id}: {e}")
        self._write_json(output_fname, created)
        self.persist_mappings()
        return created

    # ---------------- Step 5: Findings ----------------
    def create_findings(self, input_fname: str = "findings.json", output_fname: str = "new_findings.json"):
        findings = self._load_json(input_fname)
        if not findings:
            self.log("No findings.json found or file empty.")
            return []
        created = []
        url = self.base_url + RESOURCE_ENDPOINTS["findings"]
        self.log(f"Creating {len(findings)} findings sequentially (so uploads attach to known new IDs).")
        for obj in findings:
            old_id = obj.get("id")
            payload = strip_server_fields(obj)
            payload = self.remap_object_refs(payload)
            try:
                r = retry_request(self.session, "post", url, headers=None, json_payload=payload,
                                  max_retries=self.max_retries, backoff_factor=self.backoff_factor)
                if r is not None and r.status_code in (200,201):
                    try:
                        j = r.json()
                    except Exception:
                        j = {}
                    new_id = j.get("id")
                    with self.lock:
                        if old_id is not None:
                            self.mapping_store["findings"][int(old_id)] = int(new_id) if new_id is not None else None
                    self.log(f"Created finding old_id={old_id} -> new_id={new_id}")
                    # attachments: backup_dir/documents/findings/<old_id>/*
                    docs_folder = os.path.join(self.backup_dir, "documents", "findings", str(old_id))
                    if os.path.isdir(docs_folder):
                        files = [os.path.join(docs_folder, f) for f in os.listdir(docs_folder) if os.path.isfile(os.path.join(docs_folder, f))]
                        self.log(f"Found {len(files)} attachments for finding old_id={old_id}; uploading...")
                        for fpath in files:
                            fname, status, resp = self._upload_file_to_resource("findings", new_id, fpath)
                            self.log(f"  upload {fname}: status={status}")
                    else:
                        self.log(f"No attachments folder for finding old_id={old_id} at {docs_folder}")
                    created.append(j)
                else:
                    self.log(f"Failed to create finding old_id={old_id}: status={getattr(r,'status_code',None)}")
            except Exception as e:
                self.log(f"Exception creating finding old_id={old_id}: {e}")
        self._write_json(output_fname, created)
        self.persist_mappings()
        return created

# ---------------- CLI ----------------
def parse_args():
    p = argparse.ArgumentParser(description="DefectDojo restore script with optional seed mappings and attachments upload.")
    p.add_argument("--base-url", "-b", default=DEFAULT_BASE_URL, help="Base URL of DefectDojo (e.g. https://demo.defectdojo.org)")
    p.add_argument("--token", "-t", required=False, help="API token (or set DD_TOKEN env var)")
    p.add_argument("--backup-dir", "-d", required=True, help="Backup directory containing JSON files and documents/ subfolder")
    p.add_argument("--mappings", "-m", help="Optional dd_id_mappings.json to load and seed mapping_store/name->id")
    p.add_argument("--concurrency", "-c", type=int, default=2, help="Concurrency for products/users create (defaults to 2)")
    p.add_argument("--max-retries", type=int, default=3, help="Max retries per HTTP request")
    p.add_argument("--backoff-factor", type=float, default=0.5, help="Backoff factor in seconds (exponential backoff)")
    p.add_argument("--no-confirm", action="store_true", help="Do not ask for confirmation before running")
    p.add_argument("--logfile", help="Optional logfile path")
    p.add_argument("--skip-products", action="store_true")
    p.add_argument("--skip-users", action="store_true")
    p.add_argument("--skip-engagements", action="store_true")
    p.add_argument("--skip-tests", action="store_true")
    p.add_argument("--skip-findings", action="store_true")
    return p.parse_args()

def ask_confirm(prompt: str) -> bool:
    try:
        ans = input(prompt + " [y/N]: ").strip().lower()
    except Exception:
        return False
    return ans in ("y", "yes", "y\n")

def main():
    args = parse_args()
    token = args.token or os.environ.get("DD_TOKEN")
    if not token:
        print("ERROR: API token not provided (use --token or set DD_TOKEN environment variable).")
        sys.exit(1)

    restorer = DojoRestorer(
        base_url=args.base_url,
        token=token,
        backup_dir=args.backup_dir,
        concurrency=args.concurrency,
        max_retries=args.max_retries,
        backoff_factor=args.backoff_factor,
        verbose=True,
        logfile=args.logfile
    )

    # Load prior mappings if provided
    if args.mappings:
        restorer.load_mappings(args.mappings)

    print("=== DefectDojo Restore with Mappings & Attachments ===")
    print(f"Base URL: {restorer.base_url}")
    print(f"Backup dir: {restorer.backup_dir}")
    if not args.no_confirm:
        if not ask_confirm("Proceed with restore (products -> users -> engagements -> tests -> findings)?"):
            print("Aborted by user.")
            sys.exit(0)

    try:
        # Step 1: products
        if not args.skip_products:
            restorer.log("STEP 1: Creating products from products.json")
            new_products = restorer.create_products(input_fname="products.json", output_fname="new_products.json")
            restorer.log(f"Created {len(new_products)} products (new_products.json).")
        else:
            restorer.log("Skipping products step.")

        # Step 2: users
        if not args.skip_users:
            restorer.log("STEP 2: Creating users from users.json")
            new_users = restorer.create_users(input_fname="users.json", output_fname="new_users.json")
            restorer.log(f"Created {len(new_users)} users (new_users.json).")
        else:
            restorer.log("Skipping users step.")

        # Step 3: engagements (remap by product/user mapping)
        if not args.skip_engagements:
            restorer.log("STEP 3: Creating engagements from engagements.json")
            new_eng = restorer.create_engagements(input_fname="engagements.json", output_fname="new_engagements.json")
            restorer.log(f"Created {len(new_eng)} engagements (new_engagements.json).")
        else:
            restorer.log("Skipping engagements step.")

        # Step 4: tests (remap and upload attachments)
        if not args.skip_tests:
            restorer.log("STEP 4: Creating tests from tests.json (and uploading attachments)")
            new_tests = restorer.create_tests(input_fname="tests.json", output_fname="new_tests.json")
            restorer.log(f"Created {len(new_tests)} tests (new_tests.json).")
        else:
            restorer.log("Skipping tests step.")

        # Step 5: findings (remap and upload attachments)
        if not args.skip_findings:
            restorer.log("STEP 5: Creating findings from findings.json (and uploading attachments)")
            new_findings = restorer.create_findings(input_fname="findings.json", output_fname="new_findings.json")
            restorer.log(f"Created {len(new_findings)} findings (new_findings.json).")
        else:
            restorer.log("Skipping findings step.")

        restorer.persist_mappings()
        restorer.log("Restore sequence completed.")
    except KeyboardInterrupt:
        restorer.log("Aborted by user (KeyboardInterrupt).")
    except Exception as e:
        restorer.log("Fatal error:", e)
        restorer.log(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()