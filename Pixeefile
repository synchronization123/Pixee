# main.py
import os
import asyncio
from typing import List, Dict, Any, Optional

from fastapi import FastAPI, Request, Query
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
import httpx
from pydantic import BaseSettings

# ---- Settings via environment variables ----
class Settings(BaseSettings):
    API_URL_BASE: str = "https://dependencytrackapi.crm.com"   # change to your DT base URL
    API_TOKEN: str = "your_token_here"
    PAGE_LIMIT: int = 100  # page size when listing projects from DT (internal)
    # concurrency for concurrent findings requests
    CONCURRENCY: int = 8

    class Config:
        env_file = ".env"

settings = Settings()

API_BASE = settings.API_URL_BASE.rstrip("/")
PROJECTS_API = f"{API_BASE}/api/v1/project"
FINDINGS_PROJECT_API = f"{API_BASE}/api/v1/finding/project"  # usage: /api/v1/finding/project/{project_uuid}
ANALYSIS_API = f"{API_BASE}/api/v1/analysis"

HEADERS = {
    "X-Api-Key": settings.API_TOKEN,
    "Accept": "application/json",
    "Content-Type": "application/json"
}

app = FastAPI()
templates = Jinja2Templates(directory="templates")

# serve static if needed (bootstrap included from CDN so not required, but keep static mount)
if not os.path.exists("static"):
    os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")


# ---- Helper async functions to call DT ----
async def fetch_all_projects_async() -> List[Dict[str, Any]]:
    """Paginate through projects and return list. Uses offset/limit pagination."""
    client_timeout = httpx.Timeout(60.0, read=120.0)
    projects = []
    offset = 0
    limit = settings.PAGE_LIMIT
    async with httpx.AsyncClient(timeout=client_timeout, headers=HEADERS) as client:
        while True:
            params = {"offset": offset, "limit": limit}
            try:
                r = await client.get(PROJECTS_API, params=params)
                r.raise_for_status()
                batch = r.json()
            except Exception as e:
                print("Error fetching projects:", e)
                break
            if not batch:
                break
            projects.extend(batch)
            if len(batch) < limit:
                break
            offset += limit
    return projects

async def fetch_findings_for_project_async(project_uuid: str, client: httpx.AsyncClient) -> Optional[List[Dict[str, Any]]]:
    """Return list of findings for project_uuid, or None on failure."""
    url = f"{FINDINGS_PROJECT_API}/{project_uuid}"
    try:
        r = await client.get(url)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        print(f"Failed to fetch findings for project {project_uuid}: {e}")
        return None

def extract_analysis_state_from_finding(f: Dict[str, Any]) -> Optional[str]:
    """Robustly retrieve analysis state from various possible shapes."""
    # try nested analysis object
    analysis = f.get("analysis")
    if isinstance(analysis, dict):
        for key in ("analysisState", "state", "analysis.state"):
            v = analysis.get(key)
            if v is not None:
                return str(v)
    # try flat fields
    for key in ("analysisState", "analysis.state", "analysis"):
        v = f.get(key)
        if v is not None:
            return str(v)
    # if nothing present, treat as blank (None)
    return None

async def compute_counts_for_projects(projects: List[Dict[str, Any]], page: int, per_page: int) -> Dict[str, Any]:
    """
    For the provided projects list (already filtered for tag), compute blank & NOT_SET counts for each project.
    This function applies pagination (server-side).
    """
    # apply pagination
    total = len(projects)
    start = (page - 1) * per_page
    end = start + per_page
    page_projects = projects[start:end]
    results = []

    semaphore = asyncio.Semaphore(settings.CONCURRENCY)
    client_timeout = httpx.Timeout(60.0, read=120.0)
    async with httpx.AsyncClient(timeout=client_timeout, headers=HEADERS) as client:

        async def work_one(proj: Dict[str, Any]):
            async with semaphore:
                proj_uuid = proj.get("uuid")
                name = proj.get("name") or proj.get("uniqueId") or proj_uuid
                findings = await fetch_findings_for_project_async(proj_uuid, client)
                blank_count = 0
                not_set_count = 0
                total_findings = 0
                if isinstance(findings, list):
                    total_findings = len(findings)
                    for f in findings:
                        state = extract_analysis_state_from_finding(f)
                        # treat missing or empty string as blank
                        if state is None or (isinstance(state, str) and state.strip() == ""):
                            blank_count += 1
                        elif state.strip().upper() == "NOT_SET":
                            not_set_count += 1
                return {
                    "uuid": proj_uuid,
                    "name": name,
                    "blank_count": blank_count,
                    "not_set_count": not_set_count,
                    "total_findings": total_findings,
                    # optional: include tags for display
                    "tags": proj.get("tags", [])
                }

        tasks = [asyncio.create_task(work_one(p)) for p in page_projects]
        completed = await asyncio.gather(*tasks)
        results.extend(completed)

    return {
        "page": page,
        "per_page": per_page,
        "total": total,
        "projects": results
    }


# ---- Routes ----

@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """Serve the Bootstrap front-end page."""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/api/projects", response_class=JSONResponse)
async def api_projects(
    page: int = Query(1, ge=1),
    per_page: int = Query(10, ge=1, le=100),
    tag: str = Query("daily monitoring")
):
    """
    Return paginated projects that have the given tag (case-insensitive contains).
    For each project on the requested page compute blank & NOT_SET counts by fetching findings.
    """
    # 1) fetch all projects
    projects = await fetch_all_projects_async()
    # 2) filter by tag: project may have 'tags' as list of strings or objects with 'name'
    filtered = []
    tag_lower = tag.strip().lower()
    for p in projects:
        tags = p.get("tags") or []
        matched = False
        # tags might be a list of strings or list of dicts
        for t in tags:
            if isinstance(t, dict):
                name = t.get("name", "")
            else:
                name = str(t)
            if tag_lower in name.lower():
                matched = True
                break
        if matched:
            filtered.append(p)

    # 3) compute counts for paginated projects
    result = await compute_counts_for_projects(filtered, page=page, per_page=per_page)
    return JSONResponse(result)


# ---- Simple health endpoint ----
@app.get("/api/health")
async def health():
    return {"status": "ok"}